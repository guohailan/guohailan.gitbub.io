{"pages":[],"posts":[{"title":"Redis主从哨兵搭建","text":"本文章基于最新版本Redis5.0.5版本进行redis主从哨兵模式搭建 Redis 简介Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件. 它支持多种类型的数据结构，如字符串（strings），散列（hashes），列表（lists），集合（sets），有序集合（sorted sets）与范围查询，bitmaps，hyperloglogs 和 地理空间（geospatial）索引半径查询. Redis 内置了复制（replication），LUA脚本（Lua scripting），LRU驱动事件（LRU eviction），事务（transactions）和不同级别的磁盘持久化（persistence），并通过Redis哨兵（Sentinel）和自动分区（Cluster）提供高可用性（high availability）.为了实现其卓越的性能，Redis采用运行在内存中的数据集工作方式.根据您的使用情况，您可以每隔一定时间将数据集导出到磁盘，或者追加到命令日志中. 您也可以关闭持久化功能，将Redis作为一个高效的网络的缓存数据功能使用. 集群规划搭建redis需要1个主节点，且还有2个从节点，所以至少需要配置3个节点。因虚拟机是2台，所以通过配置不同端口的方式，在第一台机器上启动1个主节点，第二台上启动2个从节点，3个节点上分别启动sentinel 源码编译安装首先进入redis官网下载最新版本的redis，源码编译安装,源码安装需要依赖gcc、gcc-c++，如果出现gcc：命令未找到等错误，请先安装对应的依赖 12345wget http://download.redis.io/releases/redis-5.0.5.tar.gztar -zvxf redis-5.0.5.tar.gzmake MALLOC=libc #虚拟机只分配了一核所以未使用-j参数mkdir -p /usr/local/redis #创建redis安装目录make PREFIX=/usr/local/redis install redis启动的时候会占用一个终端，这是因为没有指定redis.conf文件，启动的时候是按默认进行的。所以如果不想使其占用，我们可以修改 redis.conf 配置文件，修改daemonize no 为 daemonize yes，然后再指定配置文件启动redis服务然后启动redis进程 1/usr/local/redis/bin/redis-server /usr/local/redis/conf/redis.conf 查看端口和使用redis-cli连接redis，启动正常 集群配置master创建/usr/local/redis/conf目录，用于存放redis的配置文件。在conf目录下创建6379目录，修改好的配置文件redis.conf放到目录下 1234567port 6379 # 端口6379bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群daemonize yes # redis后台运行pidfile /var/run/redis_6379.pid # pidfile文件对应6379appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志appendfilename appendonly.aof # aof日志文件名logfile /tmp/redis-6379.log # redis日志路径 先kill掉原来验证的单节点进程，然后启动master 12/usr/local/redis/bin/redis-server /usr/local/redis/conf/6379/redis.confnetstat -anp|grep tcp|grep -E '6379' #查看监听端口是否启动 slave创建/usr/local/redis/conf目录，用于存放redis的配置文件。在conf目录下创建6379、6380目录，修改好的配置文件redis.conf放到目录下 12345678port 6379 # 端口6379、6380bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群daemonize yes # redis后台运行pidfile /var/run/redis_6379.pid # pidfile文件对应6379、6380appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志appendfilename appendonly.aof # aof日志文件名logfile /tmp/redis-6379.log # redis日志路径replicaof 192.168.31.137 6379 #master节点的ip和端口 然后启动slave 12/usr/local/redis/bin/redis-server /usr/local/redis/conf/6379/redis.conf/usr/local/redis/bin/redis-server /usr/local/redis/conf/6380/redis.conf sentinel复制安装包中的sentinel.conf文件到redis的配置文件目录下，修改配置文件,将配置文件分别复制到对应的目录 12345678port 26379 # 端口26379、26380bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群daemonize yes # redis后台运行pidfile /var/run/sentinel_26379.pid # pidfile文件对应6379、6380logfile /tmp/redis-26379.log # redis日志路径sentinel down-after-milliseconds mymaster 30000 #master或slave多长时间（默认30秒）不能使用后标记为s_down状态sentinel monitor mymaster 192.168.31.137 6379 2 #监听的master的集群名和节点，这个后面的数字2,是指当有两个及以上的sentinel服务检测到master宕机，才会去执行主从切换的功能sentinel parallel-syncs mymaster 1 #指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为replication而不可用 启动sentinel 12/usr/local/redis/bin/redis-sentinel /usr/local/redis/conf/6379/sentinel.conf/usr/local/redis/bin/redis-sentinel /usr/local/redis/conf/6380/sentinel.conf 查看sentinel启动状态 故障迁移测试连接redis集群/usr/local/redis/bin/redis-cli -c -h 192.168.31.226 -p 6379,查询当前master信息，当前集群有2个slave模拟当前master故障，杀掉master进程再次连接redis集群发现master已经进行了重新选举，选出了新的master","link":"/2020/06/06/Redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"title":"Redis集群搭建","text":"Redis3.0上加入了cluster模式，实现的redis的分布式存储，本文主要介绍Redis集群的相关搭建。 集群特点redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。 Redis-Cluster采用无中心结构,它的特点如下： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。 工作方式： 在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是cluster，可以理解为是一个集群管理的插件。当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点A1都宕机了，那么该集群就无法再提供服务了。 集群规划搭建redis最少需要6个节点。因虚拟机是2台，所以通过配置不同端口的方式，在第一台机器上启动3个节点，第二台上启动3个节点 IP 端口 192.168.31.226 6379 192.168.31.226 6380 192.168.31.226 6381 192.168.31.137 6379 192.168.31.137 6380 192.168.31.137 6381 集群配置本文默认认为已经会安装redis，搭建部分不再赘述。需要可参考之前文章Redis主从哨兵搭建安装部分 创建/usr/local/redis/conf_cluster目录，用于存放redis的配置文件。在conf_cluster目录下创建6379、6380、6381目录，修改好的配置文件redis.conf分别放到对应目录下 123456789port 6379 # 端口6379 6380 6381bind 192.168.31.226 # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群daemonize yes # redis后台运行cluster-enabled yes # 开启集群pidfile &quot;/usr/local/redis/conf_cluster/6379/redis.pid&quot; # pidfile文件对应6379 6380 6381appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志dir &quot;/usr/local/redis/conf_cluster&quot; # 设置redis数据写入目录appendfilename appendonly.aof # aof日志文件名logfile /tmp/redis-6379.log # redis日志路径对应6379 6380 6381 启动redis全部节点 1234/usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6379/redis.conf/usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6380/redis.conf/usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6381/redis.confnetstat -anp|grep tcp|grep -E '6379|6380|6381' #查看监听端口是否启动 创建集群用redis-cli创建整个redis集群(redis5以前的版本集群是依靠ruby脚本redis-trib.rb实现) 1/usr/local/redis/bin/redis-cli -a 1234 --cluster create --cluster-replicas 1 192.168.31.226:6379 192.168.31.226:6380 192.168.31.226:6381 192.168.31.137:6379 192.168.31.137:6380 192.168.31.137:6381 # -a 表示使用密码 --cluster-replicas 1表示为每个主节点创建一个副本 使用redlis-cli连接任意节点，cluster info 和 cluster nodes可查看当前集群状态，此时集群搭建成功","link":"/2020/06/18/Redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"title":"Trojan-go搭建","text":"使用trojan-go进行搭建。 什么是Trojan-go使用Go实现的完整Trojan代理，与Trojan协议以及Trojan版本的配置文件格式兼容。安全，高效，轻巧，易用。详见官网 域名准备域名申请可以去腾讯云或者阿里云等国内大厂申请（缺点就是需要花钱），这里我是去的免费域名商freenom申请的http://www.freenom.com/zh/index.html，不过访问这个地址很慢，所以可能需要提前出去。 域名申请首先注册账号，基本信息里实测国家选择美国以外的地区可能无法注册成功，所以这里我选择的洛杉矶 输入希望申请的域名，点击检查可用性 选择心仪的域名后缀，点击现在获取 域名时长我们一般选择最长时间1年，然后将VPS的IP填入解析地址（也可以现在不填，申请完成后在My Domains里面进行填写） 域名测试默认的DNS生效时间比较长，大概半小时左右。本地能ping通域名，返回的地址为希望的IP，表示域名申请成功 证书申请一键搭建脚本可以自动申请证书（有效期3个月），证书申请步骤可以跳过 证书申请也是本着不花钱的原则，选择https://freessl.cn/ 因为只有亚洲诚信可以支持双域名，而且有效期可以1年，所以选择亚洲诚信，点击创建免费的SSL证书 填入个人邮箱，然后点击创建 登录FreeSSL的账号，安装亚洲诚信的KeyManager 点击继续后，会自动带起KeyManager，生成秘钥 回到浏览器，在浏览器上会提示DNS验证 这时我们回到freenom点击My Domains，点击Manage Domain，选择Manage Freenom DNS 将证书生成的信息填入DNS的解析里面，Type选择TXT，然后点击保存 回到freessl页面，点击配置完成（可能有30分钟左右延迟），检测一下，当检测通过，点击验证即可签发证书，证书签发以后，可以使用https访问一下自己的域名，可以看到证书是有效的 Torjan-go搭建此处我使用的是Centos7的系统 首先安装依赖包 1yum update -y &amp;&amp; yum install -y curl 安装BBR加速1wget -N --no-check-certificate &quot;https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh&quot; &amp;&amp; chmod +x tcp.sh &amp;&amp; ./tcp.sh 选择需要的BBR加速内核，我的服务器默认已经安装了BBR内核，因此我直接选择4开启 一键安装trojan-go12#安装/更新source &lt;(curl -sL https://git.io/trojan-install) 根据提示安装完毕后，可以使用trojan执行管理管理 或者通过域名在web端进行管理，可以查看当前负载以及添加和用户管理（admin密码可以通过上一图的web管理进行重置） 12#卸载source &lt;(curl -sL https://git.io/trojan-install) --remove CND加速此步骤目的为降低网络延迟，不配置也不影响使用，不过墙裂推荐，速度快了不止一倍 如果要使用CDN加速，需要将trojan切换为trojan-go（脚本默认搭建为trojan），Trojan协议本身不带加密，安全性依赖外层的TLS。但流量一旦经过CDN，TLS对CDN是透明的。其服务提供者可以对TLS的明文内容进行审查。如果你使用的是不可信任的CDN（任何在中国大陆注册备案的CDN服务均应被视为不可信任），请务必开启Shadowsocks AEAD对Webosocket流量进行加密，以避免遭到识别和审查。 cloudflare创建反代理加速注册登录cloudflare，根据提示拿到Cloudflare提供的DNS服务器 回到域名提供商，将域名的DNS解析到Cloudflare 配置完成后，可以在cloudflare首页看到域名是有效的，此时如果ping域名的话，是能看到域名解析的ip已经不是vps的ip了 开启CDN，并设置CDN点击DNS，代理状态为黄色，表示代理成功 点击页面中的 SSL/TLS 进入如下界面并设置如图所示：（重要） websocket开启若是需要Trojan套用CDN，也就是必须开启 websocket 首先找到trojan-go的配置文件，然后进行备份 在配置文件下面添加这2段，更多配置详情见trojan-go官方文档 12345&quot;websocket&quot;: { &quot;enabled&quot;: true, &quot;path&quot;: &quot;/DFE4545DFDED/&quot;, &quot;host&quot;: &quot;域名&quot;} host是主机名，一般填写域名。客户端host是可选的，填写你的域名。如果留空，将会使用remote_addr填充。 path指的是websocket所在的URL路径，必须以斜杠(“/“)开始。路径并无特别要求，满足URL基本格式即可，但要保证客户端和服务端的path一致。path应当选择较长的字符串，以避免遭到GFW直接的主动探测。 客户端的host将包含在Websocket的握手HTTP请求中，发送给CDN服务器，必须有效；服务端和客户端path必须一致，否则Websocket握手无法进行。 12345&quot;mux&quot;: { &quot;enabled&quot;: true, &quot;concurrency&quot;: 8, &quot;idle_timeout&quot;: 60} 启用多路复用不会增加你的链路速度（甚至会有所减少），而且可能会增加服务器和客户端的计算负担。可以粗略地理解为，多路复用牺牲网络吞吐和CPU功耗，换取更低的延迟。在高并发的情景下，如浏览含有大量图片的网页时，或者发送大量UDP请求时，可以提升使用体验。 concurrency是每个TLS连接最多可以承载的TCP连接数。这个数值越大，每个TLS连接被复用的程度就更高，握手导致的延迟越低。但服务器和客户端的计算负担也会越大，这有可能使你的网络吞吐量降低。如果你的线路的TLS握手极端缓慢，你可以将这个数值设置为-1，Trojan-Go将只进行一次TLS握手，只使用唯一的一条TLS连接进行传输。 idle_timeout指的是每个TLS连接空闲多长时间后关闭。设置超时时间，可能有助于减少不必要的长连接存活确认(Keep Alive)流量传输引发GFW的探测。你可以将这个数值设置为-1，TLS连接在空闲时将被立即关闭。 windows客户端配置编辑连接，勾选Mux，Type选择ws，host和Path和服务端配置文件保持一致 windows配置Qv2ray及插件安装官网下载最新版本,以及trojan-go插件QvPlugin-Trojan-Go，插件放置在Qv2ray安装目录的plugins目录下，启动Qv2ray的时候会自动加载插件 trojan-go核心下载官网下载最新版本，为了方便管理，在Qv2ray安装目录下创建了个trojan-go目录，将文件放置在此处 v2ray-core下载官网下载最新版本，为了方便管理，在Qv2ray安装目录下创建了个core目录，将文件放置在此处 Qv2ray配置插件配置运行Qv2ray客户端，点击插件，选择插件QvPlugin-Trojan-Go，点击设定，点击Browser，选择上一步下载的trojan-go核心 v2ray-core配置点击首选项，选择内核设置，选择上一步下载的v2ray-core 新建连接点击新建，配置如图 验证访问google，能正常访问，在开启的Trojan和v2rayN都能看到访问的日志，搭建成功 安卓配置下载Trojan安卓客户端（一般选择*-universal-release.apk）https://github.com/trojan-gfw/igniter/releases，安装到手机即可","link":"/2021/01/23/Trojan-go%E6%90%AD%E5%BB%BA/"},{"title":"Trojan搭建","text":"使用trojan原版进行搭建。 什么是TrojanTrojan 是一个比较新的翻墙软件，它模仿了互联网上最常见的HTTPS协议，以诱骗 GFW 认为它就是 HTTPS，从而不被识别。所谓魔高一尺道高一丈，墙在不断往上砌，那工具也得跟着变了。Trojan 工作在 443 端口，并且处理来自外界的 HTTPS 请求，如果是合法的 Trojan 请求，那么为该请求提供服务，否则将该流量转交给 WEB 服务器 Nginx，由 Nginx 为其提供服务。基于这个工作过程可以知道，Trojan 的一切表现均与 Nginx 一致，不会引入额外特征，从而达到无法识别的效果。当然，为了防止恶意探测，我们需要将 80 端口的流量全部重定向到 443 端口，并且服务器只暴露 80 和 443 端口，这样可以使得服务器与常见的 WEB 服务器表现一致。 域名准备域名申请可以去腾讯云或者阿里云等国内大厂申请（缺点就是需要花钱），这里我是去的免费域名商freenom申请的http://www.freenom.com/zh/index.html，不过访问这个地址很慢，所以可能需要提前出去。 域名申请首先注册账号，基本信息里实测国家选择美国以外的地区可能无法注册成功，所以这里我选择的洛杉矶 输入希望申请的域名，点击检查可用性 选择心仪的域名后缀，点击现在获取 域名时长我们一般选择最长时间1年，然后将VPS的IP填入解析地址（也可以现在不填，申请完成后在My Domains里面进行填写） 域名测试默认的DNS生效时间比较长，大概半小时左右。本地能ping通域名，返回的地址为希望的IP，表示域名申请成功 证书申请证书申请也是本着不花钱的原则，选择https://freessl.cn/ 因为只有亚洲诚信可以支持双域名，而且有效期可以1年，所以选择亚洲诚信，点击创建免费的SSL证书 填入个人邮箱，然后点击创建 登录FreeSSL的账号，安装亚洲诚信的KeyManager 点击继续后，会自动带起KeyManager，生成秘钥 回到浏览器，在浏览器上会提示DNS验证 这时我们回到freenom点击My Domains，点击Manage Domain，选择Manage Freenom DNS 将证书生成的信息填入DNS的解析里面，Type选择TXT，然后点击保存 回到freessl页面，点击配置完成（可能有30分钟左右延迟），检测一下，当检测通过，点击验证即可签发证书，证书签发以后，可以使用https访问一下自己的域名，可以看到证书是有效的 Torjan搭建此处我使用的是Centos7的系统 首先安装依赖包 1yum update -y &amp;&amp; yum install sudo newt curl -y &amp;&amp; sudo -i 安装BBR加速1wget -N --no-check-certificate &quot;https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh&quot; &amp;&amp; chmod +x tcp.sh &amp;&amp; ./tcp.sh 选择需要的BBR加速内核，我的服务器默认已经安装了BBR内核，因此我直接选择4开启 安装Trojan下载一键安装脚本 1curl -O https://raw.githubusercontent.com/atrandys/trojan/master/trojan_mult.sh &amp;&amp; chmod +x trojan_mult.sh &amp;&amp; ./trojan_mult.sh 根据提示信息填入域名和密码即可搭建完成，搭建完成后访问域名会出现一个网站 windows配置脚本执行完成后会在/usr/src/trojan-cli/trojan-cli.zip目录下生成带有配置文件的windows软件，下载到本地解压后双击trojan.exe直接运行即可，详细配置见config.json 安装v2rayN到https://github.com/2dust/v2rayN/releases下载v2rayN客户端，此处下载的是稳定版 解压后运行软件，点击服务器，添加Socks服务器，填入地址如图 验证访问google，能正常访问，在开启的Trojan和v2rayN都能看到访问的日志，搭建成功 安卓配置下载Trojan安卓客户端（一般选择*-universal-release.apk）https://github.com/trojan-gfw/igniter/releases，安装到手机即可","link":"/2021/01/17/Trojan%E6%90%AD%E5%BB%BA/"},{"title":"calico网络和flannel对比","text":"本文介绍了k8s常见2中方案的差异，以及各自的优势 Calico什么是CalicoCalico 是一个三层的数据中心网络方案,而且方便集成 OpenStack 这种 IaaS 云架构,能够提供高效可控的 VM、容器、裸机之间的通信。 Calico网络基本架构Calico BGP模式在小规模集群中可以直接互联,在大规模集群中可以通过额外的BGP route reflector来完成。Calico是一个基于BGP的纯三层的网络方案,与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。Calico在每个计算节点都利用Linux Kernel实现了一个高效的vRouter来负责数据转发。每个vRouter都通过BGP1协议把在本节点上运行的容器的路由信息向整个Calico网络广播,并自动设置到达其他节点的路由转发规则。Calico利用了Linux内核原生的路由和iptables防火墙功能。 进出各个容器、虚拟机和物理主机的所有流量都会在路由到目标之前遍历这些内核规则。 主要组件 Felix：Calico agent,跑在每台需要运行workload的节点上,主要负责配置路由及ACLs等信息来确保endpoint的连通状态； etcd：分布式键值存储,主要负责网络元数据一致性,确保Calico网络状态的准确性； **BGPClient(BIRD)**：主要负责把Felix写入kernel的路由信息分发到当前Calico网络,确保workload间的通信的有效性； **BGP Route Reflector(BIRD)**：大规模部署时使用,摒弃所有节点互联的mesh模式,通过一个或者多个BGPRoute Reflector来完成集中式的路由分发； CalicoCtl：允许从命令行界面配置实现高级策略和网络。 Flannel什么是FlannelFlannel是由CoreOS开发的项目,可能是最直接和最受欢迎的CNI插件。它是容器编排系统中最成熟的网络结构示例之一,旨在实现更好的容器间和主机间网络。随着CNI概念的兴起,Flannel CNI插件算是早期的入门。 Flannel网络基本架构Flannel首先创建了一个名为Flannel0的网桥,而且这个网桥的一端连接docker0网桥,另一端连接一个叫作Flanneld的服务进程。Flanneld进程上连etcd,利用etcd来管理可分配的IP地址段资源,同时监控etcd中每个Pod的实际地址,并在内存中建立了一个Pod节点路由表；Flanneld进程下连docker0和物理网络,使用内存中的Pod节点路由表,将docker0发给它的数据包包装起来,利用物理网络的连接将数据包投递到目标Flanneld上,从而完成Pod到Pod之间的直接地址通信。 对比Calico整个过程中始终都是根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和Flannel比起来效率就会快多了。由于Flannel几乎是最早的跨网络通信解决方案，其他的方案都可以被看做是Fannel的某种改进版。 Calico的设计比较新颖，Flannel的Host-Gateway模式之所以不能跨二层网络，是因为它只能修改主机的路由，Calico把改路由表的做法换成了标准的BGP路由协议。相当于在每个节点上模拟出一个额外的路由器，由于采用的是标准协议，Calico模拟路由器的路由表信息就可以被传播到网络的其他路由设备中，这样就实现了在三层网络上的高速跨节点网络。不过在现实中的网络并不总是支持BGP路由的，因此Calico也设计了一种IPIP模式，使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中的，因此它的速度理论上比VxLAN快一点点，但安全性更差。 K8S为什么会出现各种网络方案 由于Docker等容器工具只是利用内核的网络Namespace实现了网络隔离，各个节点上的容器是在所属节点上自动分配IP地址的，从全局来看，这种局部地址就像是不同小区里的门牌号，一旦拿到一个更大的范围上看，就可能是会重复的。 为了解决这个问题，Flannel设计了一种全局的网络地址分配机制，即使用Etcd来存储网段和节点之间的关系，然后Flannel配置各个节点上的Docker（或其他容器工具），只在分配到当前节点的网段里选择容器IP地址。","link":"/2020/06/10/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"title":"goftp 125 Data connection already open问题","text":"记录一次使用goftp包文件上传异常的问题 问题现象平常工作中，经常会涉及到ftp文件上传，所以使用第三方库github.com/dutchcoders/goftp，撸了个ftp上传脚本。之前使用都是正常上传，今天使用突然发现无法无法使用了，抛出的异常为125 Data connection already open; transfer starting，ftp服务器上文件名已经存在但是大小为0kb，因为编译成了exe格式，所以不涉及代码改动，顿时觉得比较奇怪，将代码又重新撸了一遍，未发现明显异常，报错依旧。 定位过程在网上也没查到啥有用信息，只有一个ftp异常码的解释125 Data connection already open; transfer starting. 资料连接已经打开，开始传送资料. 。后查阅ftp相关资料，ftp在文件上传的时候，是通过发送STOR命令来实现的，然后期待ftp返回150，是一个成功的响应码，后续会上传成功。但是现在只返回了一个125。查看goftp源码ftp.go，当ftp连接上以后，会返回一个状态，如果不是以StatusFileOK开头，则return 继续查看status.go中定义的StatusFileOK，定义的值为150，所以返回125会抛出异常。 原因经过一下午的排查，原因找到了，应该是国庆期间公司对ftp的服务器进行了升级。IIS7对响应码处理做了调整。http://support.microsoft.com/kb/2505047 ，IIS7前，对APPE，STOU ，STOR命令，passive模式响应125，active模式响应150。IIS7.5后不考虑连接模式了，只考虑当前的连接状态，未连接响应150，已连接响应125。 解决办法使用了一个比较粗暴的办法，直接将源码status.go中的StatusFileOK=&quot;150&quot;改为StatusFileOK=&quot;125&quot;，重新编译脚本之后上传正常。因只在本地使用，未做兼容性测试。仅此记录","link":"/2020/10/19/goftp-125-Data-connection-already-open%E9%97%AE%E9%A2%98/"},{"title":"kubeadm搭建k8s 1.18","text":"本文主要介绍k8s搭建，使用官方推荐工具kubeadm进行搭建，ETCD独立部署 docker 安装添加阿里镜像源地址 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装docker出现如下错误,当前服务器上安装的containerd.io版本为1.2.0-3.el7版本过低 1234567891011121314[root@guohailan3 ~]# yum -y install docker-ce上次元数据过期检查：0:00:07 前，执行于 2020年05月24日 星期日 03时42分48秒。错误： 问题: package docker-ce-3:19.03.9-3.el7.x86_64 requires containerd.io &gt;= 1.2.2-3, but none of the providers can be installed - cannot install the best candidate for the job - package containerd.io-1.2.10-3.2.el7.x86_64 is excluded - package containerd.io-1.2.13-3.1.el7.x86_64 is excluded - package containerd.io-1.2.13-3.2.el7.x86_64 is excluded - package containerd.io-1.2.2-3.3.el7.x86_64 is excluded - package containerd.io-1.2.2-3.el7.x86_64 is excluded - package containerd.io-1.2.4-3.1.el7.x86_64 is excluded - package containerd.io-1.2.5-3.1.el7.x86_64 is excluded - package containerd.io-1.2.6-3.3.el7.x86_64 is excluded(尝试添加 '--skip-broken' 来跳过无法安装的软件包 或 '--nobest' 来不只使用最佳选择的软件包) 升级containerd.io，再次进行安装 12wget https://download.docker.com/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpmyum install containerd.io-1.2.6-3.3.el7.x86_64.rpm 配置开机启动和启动docker 12systemctl enable dockersystemctl start docker 优化docker参数 官方文档表示，更改设置，令容器运行时和kubelet使用systemd作为cgroup驱动，以此使系统更为稳定。 12345678910111213141516tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;registry-mirrors&quot;: [&quot;https://bk6kzfqm.mirror.aliyuncs.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]}EOFsystemctl daemon-reload # 重新加载systemctl restart docker # 重启docker kubernet 安装安装kubectl1234567891011121314#无法连接google，配置了阿里的源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOFyum install -y kubectlsetenforce 0 #暂时关闭selinuxsed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux #Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动。swapoff -ayum install -y kubeadm kubelet #安装kubeadm kubelet工具 准备证书证书需要在一台机器上生成，拷贝到别的机器上 准备证书管理工具1234567891011wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 生成ETCD的TLS 秘钥和证书1234567891011121314151617181920212223# 创建 CA 配置文件mkdir ssl &amp;&amp; cd sslcfssl print-defaults csr &gt; csr.jsoncat &gt; config.json &lt;&lt;EOF{&quot;signing&quot;: { &quot;default&quot;: { &quot;expiry&quot;: &quot;8760h&quot; }, &quot;profiles&quot;: { &quot;kubernetes&quot;: { &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;8760h&quot; } }}}EOF config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证； client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证； 创建ca证书请求123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF{ &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;GuangDong&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ]}EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 创建CA证书和私钥1cfssl gencert -initca ca-csr.json | cfssljson -bare ca 创建 etcd 证书签名请求1234567891011121314151617181920212223cat &gt; etcd-csr.json &lt;&lt;EOF{ &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.31.48&quot;, &quot;192.168.31.137&quot;, &quot;192.168.31.226&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;GuangDong&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ]}EOF hosts 字段指定授权使用该证书的 etcd 节点 IP； 每个节点IP 都要在里面 或者 每个机器申请一个对应IP的证书 生成 etcd 证书和私钥1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 将证书拷贝到所有服务器的指定目录12mkdir -p /etc/etcd/sslcp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/ 安装ETCD准备二进制文件1234wget https://github.com/coreos/etcd/releases/download/v3.4.9/etcd-v3.4.9-linux-amd64.tar.gztar -vxf etcd-v3.4.9-linux-amd64.tar.gzcp etcd-v3.4.9-linux-amd64/etcd* /usr/bin/chmod +x /usr/bin/etcd* 部署环境变量12345export NODE_NAME=&quot;etcd-host1&quot; #当前部署的机器名称(随便定义，只要能区分不同机器即可)export NODE_IP=&quot;192.168.31.48&quot; # 当前部署的机器 IPexport export NODE_IPS=&quot;192.168.31.48 192.168.31.137 192.168.31.226&quot; # etcd 集群所有机器 IP# etcd 集群间通信的IP和端口export ETCD_NODES=&quot;etcd-host1=https://192.168.31.48:2380,etcd-host2=https://192.168.31.137:2380,etcd-host3=https://192.168.31.226:2380&quot; 创建etcd的systemd unit文件12345678910111213141516171819202122232425262728293031323334mkdir -p /var/lib/etcd # 必须先创建工作目录cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/bin/etcd \\\\ --name=${NODE_NAME} \\\\ --cert-file=/etc/etcd/ssl/etcd.pem \\\\ --key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\\\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --trusted-ca-file=/etc/etcd/ssl/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\\\ --initial-advertise-peer-urls=https://${NODE_IP}:2380 \\\\ --listen-peer-urls=https://${NODE_IP}:2380 \\\\ --listen-client-urls=https://${NODE_IP}:2379,http://127.0.0.1:2379 \\\\ --advertise-client-urls=https://${NODE_IP}:2379 \\\\ --initial-cluster-token=etcd-cluster-0 \\\\ --initial-cluster=${ETCD_NODES} \\\\ --initial-cluster-state=new \\\\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF –name：方便理解的节点名称，默认为 default，在集群中应该保持唯一，可以使用 hostname –data-dir：服务运行数据保存的路径，默认为 ${name}.etcd –snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘 –heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms –eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms –listen-peer-urls：和同伴通信的地址，比如 http://ip:2380，如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！ –listen-client-urls：对外提供服务的地址：比如 http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和 etcd 交互 –advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点 –initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点 –initial-cluster：集群中所有节点的信息，格式为 node1=http://ip1:2380,node2=http://ip2:2380,…。注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值 –initial-cluster-state：新建集群的时候，这个值为 new；假如已经存在的集群，这个值为 existing –initial-cluster-token：创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误 启动 etcd 服务12345mv etcd.service /etc/systemd/system/systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 验证服务并配置etcdctl工具1etcdctl --endpoints=&quot;https://192.168.31.48:2379,https://192.168.31.137:2379,https://192.168.31.226:2379&quot; --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem endpoint health 为了后续操作方便可配置alias 1alias etcdctl='etcdctl --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=&quot;https://192.168.31.48:2379,https://192.168.31.137:2379,https://192.168.31.226:2379&quot;' 初始化master创建master配置文件1234567891011121314151617181920212223cat &gt; /etc/kubernetes/config.yaml &lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1beta1# 国内不能访问 Google，修改为阿里云imageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfiguration### etcd 配置及秘钥 ###etcd: external: endpoints: - https://192.168.31.48:2379 - https://192.168.31.137:2379 - https://192.168.31.226:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcd### calico 网络插件的子网 ###networking: podSubnet: &quot;10.0.0.0/16&quot; serviceSubnet: &quot;10.96.0.0/12&quot;###k8s的版本###kubernetesVersion: 1.18.0EOF 初始化master可查看依赖的镜像版本 1kubeadm config images list --kubernetes-version=v1.18.0 1kubeadm init --config /etc/kubernetes/config.yaml #主节点进行初始化 出现如下提示表示配置成功 1234567891011121314Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.31.226:6443 --token v7b8zo.5hc1au8vl96xqyie \\ --discovery-token-ca-cert-hash sha256:908597386cd0311f24e2d7c95e40559e3137523078e69e1262643c6161abc10a 如果出现如下错误，需要按照上图提示，配置kube/config 多master初始化如果希望部署多master需要用下面的配置文件，token和tokenTTL在初始化第一台master的时候注释掉 12345678910111213141516171819202122232425262728293031323334353637383940cat &gt; /etc/kubernetes/config.yaml &lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1beta1# 国内不能访问 Google，修改为阿里云imageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfiguration### etcd 配置及秘钥 ###etcd: external: endpoints: - https://192.168.31.48:2379 - https://192.168.31.137:2379 - https://192.168.31.226:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcd### calico 网络插件的子网 ###networking: podSubnet: &quot;10.0.0.0/16&quot; serviceSubnet: &quot;10.96.0.0/12&quot;###k8s的版本###kubernetesVersion: 1.18.0######多master配置########下面的token是master1节点初始化完成后，join得到的token值token: &quot;1gddb4.cs1chtdrk5r9aa0i&quot;tokenTTL: &quot;0s&quot;#是kubeadm帮我们apiserver生成对外服务的证书用的。因为外部访问apiserver是通过负载均衡实现的，所以作为服务端提供的证书中应该写的hosts是负载均衡的地址。apiServerCertSANs:#允许访问apiserver的地址- 192.168.30.189 #apiserver的负载均衡ip，或者是slb的ip- k8s-master1- k8s-slave1- k8s-slave2- 192.168.31.48- 192.168.31.137- 192.168.31.226apiServerExtraArgs: apiserver-count: &quot;3&quot; endpoint-reconciler-type: leaseEOF calico安装安装calico 123kubectl apply -f https://docs.projectcalico.org/v3.15/manifests/calico.yamlhttps://docs.projectcalico.org/manifests/calico-etcd.yamlkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 查看节点启动详情，calico3个进程都启动完毕 添加node节点在另一台节点执行下面命令，加入集群 1kubeadm join 192.168.31.226:6443 --token v7b8zo.5hc1au8vl96xqyie --discovery-token-ca-cert-hash sha256:908597386cd0311f24e2d7c95e40559e3137523078e69e1262643c6161abc10a 出现如下提示表示加入成功 在master执行kubectl get nodes，看到node状态都是Ready表示搭建完成","link":"/2020/08/20/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"title":"nginx配置限流","text":"系统设计时一般会预估负载，当系统暴露在公网中时，恶意攻击或正常突发流量等都可能导致系统被压垮，而限流就是保护措施之一。限流即控制流量，本文将记录 Nginx 的一种速率限流设置。 ngx_http_limit_req_modulengx_http_limit_req_module模块提供限制请求处理速率能力，使用了漏桶算法(leaky bucket)，即能够强行保证请求的实时处理速度不会超过设置的阈值。算法思想是： 令牌以固定速率产生，并缓存到令牌桶中； 令牌桶放满时，多余的令牌被丢弃； 请求要消耗等比例的令牌才能被处理； 令牌不够时，请求被缓存。 配置示例123456789101112http { #表示设置一块10m的共享内存来保存键值得状态，键为$uri，平均处理的频率不超过每秒1次 limit_req_zone $uri zone=one:10m rate=1r/s; ... server { ... location /limittest { #使用共享内存one，同时允许超过频率限制的请求数不多于5个,nodelay表示不希望超过的请求被延迟 limit_req zone=one burst=5 nodelay; #如果超出限制，返回429的返回码 limit_req_status 429; } 参数解析设置共享内存区域和请求的最大突发大小。如果请求速率超过为某个区域配置的速率，则它们的处理会延迟（不配置nodelay参数情况下），从而使请求按指定速率处理。过多的请求被延迟，直到它们的数量超过最大突发大小，在这种情况下请求被终止并出现错误。默认情况下，最大突发大小等于零。 limit_req_zone key zone=zone:size rate=rate; key 若客户的请求匹配了key，则进入zone。可以是文本、变量，通常为Nginx变量。如$binary_remote_addr(客户的ip)，$uri(不带参数的请求地址)，$request_uri(带参数的请求地址)，$server_name(服务器名称)。支持组合使用，使用空格隔开。 zone 使用zone=one，指定此zone的名字为one。 size 在zone=name后面紧跟:size，指定此zone的内存大小。如zone=name:10m，代表name的共享内存大小为10m。通常情况下，1m可以保存16000个状态。 rate 使用rate=1r/s，限制平均1秒不超过1个请求。使用rate=1r/m，限制平均1分钟不超过1个请求。如果需要每秒小于一个请求的速率，则按每分钟请求（r/m）指定。 验证编辑nginx配置文件nginx.conf,加入限流配置，先执行nginx -t检查配置是否ok，如果返回success表示文件检查ok 然后执行nginx -s reload重启nginx，使配置生效浏览器访问配置了限流的url进行验证，可以看到有429返回，表示限流配置生效 跳转自定义页面nginx触发限流后返回的页面非常不友好，因此我们可以开启自定义error页面，使返回页面更加友好 下面这段代码可以配置在server级别，也可以配置在location级别，为了不影响别的文根，此处我们在location下配置 12345678location /limittest { ... # 关键参数：这个变量开启后，我们才能自定义错误页面，当后端返回429，nginx拦截错误定义错误页面 fastcgi_intercept_errors on; error_page 429 /429.html; #error_page 429 = http://www.test.com/429.html; 也可以重定向到另一个url ...} 注意:如果nginx返回的是本地的html，则页面状态码和定义的相同。若是跳转到某个url，页面状态码为302。 nginx常见内置变量 $args #这个变量等于请求行中的参数。 $content_length #请求头中的Content-length字段。 $content_type #请求头中的Content-Type字段。 $document_root #当前请求在root指令中指定的值。 $host #请求主机头字段，否则为服务器名称。 $http_user_agent #客户端agent信息 $http_cookie #客户端cookie信息 $limit_rate #这个变量可以限制连接速率。 $request_body_file #客户端请求主体信息的临时文件名。 $request_method #客户端请求的动作，通常为GET或POST。 $remote_addr #客户端的IP地址。 $remote_port #客户端的端口。 $remote_user #已经经过Auth Basic Module验证的用户名。 $request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 $query_string #与$args相同。 $scheme #HTTP方法（如http，https）。 $server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 $server_name #服务器名称。 $server_port #请求到达服务器的端口号。 $request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。 $uri #不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。 $document_uri #与$uri相同。","link":"/2020/11/08/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"title":"prometheus+grafana监控搭建","text":"本文主要介绍prometheus+grafana+consul方案监控系统的搭建，本次搭建采用虚拟机方式进行搭建，采取consul进行服务注册 prometheus安装与配置介绍与基本架构prometheus是由谷歌研发的一款开源的监控软件，目前已经被云计算本地基金会托管，是继k8s托管的第二个项目。prometheus根据配置定时去拉取各个节点的数据，默认使用的拉取方式是pull，也可以使用pushgateway提供的push方式获取各个监控节点的数据。将获取到的数据存入TSDB，一款时序型数据库。此时prometheus已经获取到了监控数据，可以使用内置的PromQL进行查询。它的报警功能使用Alertmanager提供，Alertmanager是prometheus的告警管理和发送报警的一个组件。prometheus原生的图标功能过于简单，可将prometheus数据接入grafana，由grafana进行统一管理。 安装Prometheus Server首先从官网下载Prometheus安装程序 1wget https://github.com/prometheus/prometheus/releases/download/v2.18.1/prometheus-2.18.1.linux-amd64.tar.gz 执行执行解压命令 1tar -vxf prometheus-2.18.1.linux-amd64.tar.gz 编辑解压目录下的prometheus.yml，执行命令：vi prometheus.yml进行基本配置 12345678910111213141516171819[root@guohailan1 prometheus-2.18.1.linux-amd64]# cat prometheus.yml# my global configglobal: scrape_interval: 10s #每10s采集一次数据 evaluation_interval: 10s #每10s做一次告警检测 scrape_timeout: 5s #拉取一个 target 的超时时间alerting: #Alertmanager 相关配置暂时未配置 alertmanagers: - static_configs: - targets:rule_files:scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'linux-exporter' metrics_path: /metrics static_configs: - targets: ['192.168.31.48:9100'] 配置开机启动，在CentOS8下官方推荐使用systemctl进行开机自启管理 123456789101112cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/prometheus.service[Unit]Description=PrometheusAfter=network.target[Service]Type=simpleExecStart=/bin/bash -c &quot;/prometheus/prometheus-2.18.1.linux-amd64/prometheus --web.enable-lifecycle --storage.local.retention 24h0m0s --config.file=/prometheus/prometheus-2.18.1.linux-amd64/prometheus.yml&quot;[Install]WantedBy=multi-user.targetEOF 设置自启动和启动prometheus 12systemctl enable prometheussystemctl start prometheus 浏览器端访问http://192.168.31.48:9090/graph，如果出现界面表示搭建成功,如果服务器上有防火墙，可能需要先关闭防火墙或者配置规则 node_exporter安装node-exporter用于采集服务器层面的运行指标，包括机器的loadavg、filesystem、meminfo等基础监控，类似于传统主机监控维度的zabbix-agent。node-export由prometheus官方提供、维护，不会捆绑安装，但基本上是必备的exporter。从prometheus官网下载相应版本的node_exporter 1wget https://github.com/prometheus/consul_exporter/releases/download/v0.6.0/consul_exporter-0.6.0.linux-amd64.tar.gz 配置开机启动 12345678910111213cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/node_exporter.service[Unit]Description=node_exporterAfter=network.target [Service]Type=simpleExecStart=/bin/bash -c &quot;/root/exporter/node_exporter-1.0.0-rc.1.linux-amd64/node_exporter&quot;[Install]WantedBy=multi-user.targetEOF consul安装consulConsul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.命令行超级好用的虚拟机管理软件 vgrant 也是 HashiCorp 公司开发的产品.一致性协议采用 Raft 算法,用来保证服务的高可用. 使用 GOSSIP 协议管理成员和广播消息, 并且支持 ACL 访问控制. 首先从官网上下载consul最新版本并且解压 12wget https://releases.hashicorp.com/consul/1.7.3/consul_1.7.3_linux_amd64.zipunzip consul_1.7.3_linux_amd64.zip 本案例搭建为单机版本,将下列命令写入start.sh 1consul agent -data-dir /prometheus/consul/data -bind=172.0.0.1 -datacenter=dc1 -ui -client=0.0.0.0 -server -http-port=8500 -bootstrap-expect=1 配置开机启动 123456789101112cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/consul.service[Unit]Description=consulAfter=network.target[Service]Type=simpleExecStart=/bin/bash -c &quot;/prometheus/consul/start.sh&quot;[Install]WantedBy=multi-user.targetEOF 访问http://localhost:8500 ,能出现界面表示搭建成功 consul注册与删除在服务器上使用如下命令将node_exporter,注册成功后可以看到consul界面上出现了注册的信息，Node Checks和Service Checks绿色表示node_exporter状态正常，点击可查看详情 1curl -X PUT -d '{&quot;id&quot;: &quot;192.168.31.48&quot;,&quot;name&quot;: &quot;node-exporter&quot;,&quot;address&quot;: &quot;192.168.31.48&quot;,&quot;port&quot;: 9100,&quot;tags&quot;: [&quot;guohailan1&quot;],&quot;checks&quot;: [{&quot;http&quot;: &quot;http://192.168.31.48:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot;}]}' http://192.168.31.48:8500/v1/agent/service/register 如果注册错误或者不使用了可用如下命令删除注册信息 1curl -X PUT http://192.168.31.48:8500/v1/agent/service/deregister/node-exporter ###prometheus对接consul修改prometheus配置文件vi /prometheus/prometheus-2.18.1.linux-amd64/prometheus.yml 12345678910111213141516171819202122# my global configglobal: scrape_interval: 10s evaluation_interval: 10s scrape_timeout: 5salerting: alertmanagers: - static_configs: - targets:rule_files:scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'consul-prometheus' consul_sd_configs: - server: '192.168.31.48:8500' #consul地址 services: [] relabel_configs: - source_labels: [__meta_consul_service_port] #prometheus将对匹配上的lables进行操作 regex: 9100 action: keep 详细 relabel_configs 配置及说明可以参考 relabel_config 官网说明，这里我简单列举一下里面每个 relabel_action 的作用，方便下边演示。 replace: 根据 regex 的配置匹配 source_labels 标签的值（注意：多个 source_label 的值会按照 separator 进行拼接），并且将匹配到的值写入到 target_label 当中，如果有多个匹配组，则可以使用 ${1}, ${2} 确定写入的内容。如果没匹配到任何内容则不对 target_label 进行重新， 默认为 replace。 keep: 丢弃 source_labels 的值中没有匹配到 regex 正则表达式内容的 Target 实例 drop: 丢弃 source_labels 的值中匹配到 regex 正则表达式内容的 Target 实例 hashmod: 将 target_label 设置为关联的 source_label 的哈希模块 labelmap: 根据 regex 去匹配 Target 实例所有标签的名称（注意是名称），并且将捕获到的内容作为为新的标签名称，regex 匹配到标签的的值作为新标签的值 labeldrop: 对 Target 标签进行过滤，会移除匹配过滤条件的所有标签 labelkeep: 对 Target 标签进行过滤，会移除不匹配过滤条件的所有标签 配置完成重启promethus即可看到targets上有新的node_exporter信息，其中Labels中的指标即上一步配置中的 “source_labels” grafanagrafana是一个非常酷的数据可视化平台，常常应用于显示监控数据，底层数据源可以支持influxDb、graphite、elasticSeach。 安装grafana首先还是从官网下载安装包,进行安装 1234wget https://dl.grafana.com/oss/release/grafana-7.0.1-1.x86_64.rpmsudo yum install grafana-7.0.1-1.x86_64.rpmsystemctl enable grafana-server.service #允许开机启动systemctl start grafana-server.service #启动grafana 启动服务之后：http://localhost:3000 。用户名和密码在初始化都是admin和admin 配置grafana数据源首先配置数据源,选择之前搭建的prometheus的连接串，点击保存 dashboardgrafana官网有很多大神的作品，可以直接使用grafana支持很多中导入方式，上传json文件或者直接贴json配置，这里因可以使用外网，直接复制模板ID，导入到grafana中导入完成后，即可看到报表信息","link":"/2020/06/02/prometheus+grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"title":"python分析豆瓣影评","text":"本文主要对当前热门剧《隐秘的角落》进行豆瓣短评以及评价进行可视化分析，数据抓取主要为python编写的爬虫。 本文主要分为2个部分，分别对爬虫和可视化部分进行详解，本文脚本基于python 3.8.0版本进行编写 第三方依赖包 说明 pyecharts 百度开源可视化工具，用于生成图表和词云 jieba 国内比较好用的分词工具 requests python常用HTTP 库 lxml pyhon常用XML和HTML解析库 数据抓取日志打印为了方便调试和问题定位，单独引入了logging模块进行日志打印，为方便复用，日志部分单独写到了一个文件中，创建log.py文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445import loggingimport logging.handlersclass Logger(object): level_relations = { 'debug':logging.DEBUG, 'info':logging.INFO, 'warning':logging.WARNING, 'error':logging.ERROR, 'crit':logging.CRITICAL }#日志级别关系映射 def __init__(self,filename,level='info',when='H',backCount=3,fmt='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'): #日志格式 #---------------------------- # %(asctime)s 年-月-日 时-分-秒，毫秒 # %(filename)s 文件名，不含目录 # %(pathname)s 目录名，完整路径 # %(funcName)s 函数名 # %(levelname)s 级别名 # %(lineno)d 行号 # %(module)s 模块名 # %(message)s 日志信息 # %(name)s 日志模块名 # %(process)d 进程id # %(processName)s 进程名 # %(thread)d 线程id # %(threadName)s 线程名 self.logger = logging.getLogger(filename) format_str = logging.Formatter(fmt)#设置日志格式 self.logger.setLevel(self.level_relations.get(level))#设置日志级别 sh = logging.StreamHandler()#往屏幕上输出 sh.setFormatter(format_str) #设置屏幕上显示的格式 th = logging.handlers.TimedRotatingFileHandler(filename=filename,when=when,backupCount=backCount,encoding='utf-8')#往文件里写入#指定间隔时间自动生成文件的处理器 #实例化TimedRotatingFileHandler #interval是时间间隔，backupCount是备份文件的个数，如果超过这个个数，就会自动删除，when是间隔的时间单位，单位有以下几种： # S 秒 # M 分 # H 小时、 # D 天、 # W 每星期（interval==0时代表星期一） # midnight 每天凌晨 th.setFormatter(format_str)#设置文件里写入的格式 self.logger.addHandler(sh) #把对象加到logger里 self.logger.addHandler(th) 爬虫豆瓣对未登陆账号的请求有些限制，短评只能看到前面200条，所以采取了登陆账号的方式来请求更多的数据。豆瓣返回的数据为xml，因此采用了xpath的方式来获取短评数据，然后将数据存入本地文本中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class sprider(): def __init__(self): # 设置请求头 self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko'} self.short_comments_file = sys.path[0]+&quot;\\\\Short_comments_file.txt&quot; self.score_file = sys.path[0]+&quot;\\\\score_file.txt&quot; def login(self,session): login_url = 'https://accounts.douban.com/j/mobile/login/basic' ## 账号和密码需要修改为正确的 postData = {'ck': '', 'name': 'yourname', 'password': 'passwd', 'remember': 'false', 'ticket': ''} # 从网上看到，需要先get请求一次才能成功，不然能登录能200，但是不能进行后续的get请求抛403，具体原因不详 a = session.get(login_url,headers=self.headers) b = session.post(login_url, data=postData, headers=self.headers) if b.status_code == 200: logger.logger.info(&quot;登录成功&quot;) else: # 登陆失败打印返回码和失败详情 logger.logger.error(&quot;登录失败:&quot;+str(b.status_code)+str(b.text)) sys.exit(48) def get_request(self,url,session): # get请求获取短评详情 request = session.get(url,headers=self.headers) if request.status_code == 200: # 防止乱码，将编码格式设置为utf-8 request.encode = 'utf-8' return request.text else: logger.logger.error('current status_code:'+str(request.status_code)+str(request.text)) sys.exit(44) def xpath_analysis(self, text, xpath): html = etree.HTML(text) # 使用xpath进行短评解析 result = html.xpath(xpath) return result def to_file(self, filename, data): # 采用追加的方式将内容写入文件 with open(filename, 'a', encoding='utf-8') as f: # xpath获取的data为列表，所以用遍历的方式写入 logger.logger.info(data) for i in data: f.write(i+'\\n') def init_file(self, filename): # 初始化文件，直接清空 with open(filename, 'w') as f: f.write('') 可视化可视化部分主要是对前一步抓取到的数据进行清洗和分析，分词库用的是jieba。图表生成使用的是百度开源可视化工具pyecharts，本文只用了柱状图和词云，更多高级用法见pyecharts官方文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class visualization(): def __init__(self): # 初始化文件路径 self.short_comments_file = sys.path[0]+&quot;\\\\Short_comments_file.txt&quot; self.score_file = sys.path[0]+&quot;\\\\score_file.txt&quot; self.path = sys.path[0] def analysis_score(self): evaluate = {} with open(self.score_file, 'r', encoding='utf-8') as f: for i in f.readlines(): i = re.sub(r&quot;[A-Za-z0-9\\-\\:]&quot;, &quot;&quot;, i) i = i.strip() # 去掉每行的换行符 # 如果这个没出现过，就初始化为1 if i not in evaluate: evaluate[i] = 1 logger.logger.debug(evaluate) else: # 如果已经出现过了，就在自加1 evaluate[i] += 1 logger.logger.debug(evaluate) logger.logger.info(evaluate) bar = Bar() eva = [] count = [] for k, v in evaluate.items(): if k != '': eva.append(k) count.append(v) bar.add_xaxis(eva) # 柱状图x轴 logger.logger.info('xaxis'+str(eva)) bar.add_yaxis(&quot;评价&quot;, count) # 柱状图y轴 logger.logger.info('yaxis'+str(count)) bar.set_global_opts(title_opts=opts.TitleOpts(title=&quot;隐秘的角落 豆瓣评分&quot;)) # 生成可视化图表 bar.render(self.path+&quot;\\\\score.html&quot;) def analysis_short_comment(self): cut_words = &quot;&quot; for line in open(self.short_comments_file, 'r', encoding='utf-8'): line.strip('\\n') # 正则去掉标点等无效的字符，对数据进行清洗 line = re.sub(r&quot;[A-Za-z0-9\\：\\·\\—\\，\\。\\“ \\”\\....]&quot;, &quot;&quot;, line) # cut_all=False为精确模式，cut_all=True为全词模式 seg_list = jieba.cut(line, cut_all=False) cut_words += (&quot; &quot;.join(seg_list)) all_words = cut_words.split() c = Counter() for x in all_words: if len(x) &gt; 1 and x != '\\r\\n': c[x] += 1 words = c.most_common(500) # 输出词频最高的前500词 logger.logger.debug(words) wordcloud = WordCloud() wordcloud.add(&quot;&quot;, words, word_size_range=[5, 100], shape='circle') wordcloud.set_global_opts(title_opts=opts.TitleOpts(title=&quot;隐秘的角落 短评&quot;)) wordcloud.render(self.path+&quot;\\\\short_comment.html&quot;) 主函数为了防止被豆瓣封IP，降低了采集频率采取了单线程和每次请求随机sleep 0.1s-4s的方式 1234567891011121314151617181920212223242526272829303132333435363738def run_sprider(sprider): # 初始化文件 sprider.init_file(sprider.short_comments_file) sprider.init_file(sprider.score_file) # 遍历获取数据 s = requests.session() sprider.login(s) for page_start in range(0, 500, 20): # 范围从0-500，步长为20,页面上总评论数为500+ try: delay = round(random.uniform(0.1, 4), 1) logger.logger.info('i will sleep:'+str(delay)+'s') time.sleep(delay) URL = 'https://movie.douban.com/subject/33404425/comments?start={}&amp;limit=20&amp;sort=new_score&amp;status=P'.format( page_start) logger.logger.info('current_request_url:'+URL) x = sprider.get_request(URL,s) # 短评的xpath路径 xpath = '//*[@id=&quot;comments&quot;]/div[*]/div[2]/p/span/text()' short_comment = sprider.xpath_analysis(x, xpath) sprider.to_file(sprider.short_comments_file, short_comment) # 评分的xpath路径 xpath2 = '//*[@id=&quot;comments&quot;]/div[*]/div[2]/h3/span[2]/span[2]/@title' score = sprider.xpath_analysis(x, xpath2) sprider.to_file(sprider.score_file, score) except Exception as e: logger.logger.error(e) logger.logger.info('current_page:'+page_start) sys.exit(146)if __name__ == &quot;__main__&quot;: # 数据抓取 sprider = sprider() run_sprider(sprider) # 数据分析和图标生成 c = visualization() c.analysis_score() c.analysis_short_comment() 完整源码log.pysprider.py","link":"/2020/07/05/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"title":"使用Jenkins进行持续集成","text":"使用Jenkins进行持续集成，案例在虚拟机上进行搭建，本文主要介绍环境搭建以及配置部分。 准备工作机器要求： 256 MB 内存，建议大于 512 MB 10 GB 的硬盘空间（用于 Jenkins 和 Docker 镜像） 需要安装以下软件 Java 8 ( JRE 或者 JDK 都可以) 首先从官网上下载Jenkins安装包，为方便使用这里我直接下载了war包进行安装 1wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war 从国内源下载JDK 8,并且设置JAVA_HOME 1234wget https://repo.huaweicloud.com/java/jdk/8u201-b09/jdk-8u201-linux-x64.tar.gz # 下载JDK安装包echo 'export JAVA_HOME=/usr/java/jdk1.8.0_201export PATH=$JAVA_HOME/bin:$PATH' &gt;&gt;/etc/profile # 配置java环境变量source /etc/profile # 使配置的环境变量生效 java -version查看java版本,如果出现下图结果，表示JDK安装成功 使用Jenkins执行java -jar jenkins.war --httpPort=8080启动jenkins，打开浏览器进入链接http://localhost:8080,访问jenkis 解锁Jenkins第一次访问新的Jenkins实例时，系统会要求使用自动生成的密码对其进行解锁。从Jenkins控制台日志输出中，复制自动生成的字母数字密码（在两组星号之间） 插件初始化在解锁Jenkins页面上，将此密码粘贴到管理员密码字段中，然后单击继续,此时会进入插件安装界面,根据需要进行选择，这里我直接选择了自定义安装，暂时不安装任何插件，因为没有更改源，安装很慢 安装建议的插件 - 安装推荐的一组插件，这些插件基于最常见的用例. 选择要安装的插件 - 选择安装的插件集。当你第一次访问插件选择页面时，默认选择建议的插件。 插件下载完成后即可进入主页 主从搭建先点击Manage Jenkins，然后点击Manage Nodes and Clouds点击新建节点填写slave相关信息,点击保存在Configure Global Security中启用代理Java Web Start Agent Protocol/4 (TLS 加密)配置然后根据提示，将agent.jar拷贝到slave节点目录，然后根据提示的命令运行slave此时可以看到slave已经上线 用户权限配置在一个部门内部，可能存在多个运维人员，而这些运维人员往往负责不同的项目，但是有可能他们用的又是同一个 Jenkins 的不同用户。那么我们就希望实现一个需求，能够不同的用户登录 Jenkins 以后看到不同的项目。由于jenkins默认的权限控制太过简陋，因此我们引入新的插件Role-based Authorization Strategy 插件安装首先因国内直连jenkens的插件中心速度很慢，因此先修改源站为清华大学地址https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json然后搜索插件名进行安装 权限配置重启Jenkins以后，再度打开Configure Global Security会发现多了我们刚刚插件的选项，选择Role-Based Strategy，点击保存这时在Manage Jenkins就会多出一个选项，Manage and Assign Roles在Manage Roles进行角色以及相关权限添加，有Global roles、Item roles以及Node roles可以分别进行配置然后点击Assign Roles，进行相关用户的角色授予 创建JOB点击新建job，创建一个Freestyle project的JOB点击新创建的JOB进行配置，源码管理选择GIT(需要提前安装git插件才会出来这个选项，填入git库地址，以及相关账号及密码)定时触发器可根据实际情况进行选择，然后配置构建步骤点击保存即可，我本地是部署nginx静态页面，因此直接执行shell命令进行部署 触发构建点击立即构建，即可触发构建任务，本次部署为和jenkins同一台机器上的nginx静态页面点击控制台输出，可以查看构建过程中的具日志","link":"/2020/06/10/%E4%BD%BF%E7%94%A8Jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"}],"tags":[{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"搭建文档","slug":"搭建文档","link":"/tags/%E6%90%AD%E5%BB%BA%E6%96%87%E6%A1%A3/"},{"name":"torjan","slug":"torjan","link":"/tags/torjan/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"问题定位","slug":"问题定位","link":"/tags/%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"限流","slug":"限流","link":"/tags/%E9%99%90%E6%B5%81/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"grafana","slug":"grafana","link":"/tags/grafana/"},{"name":"监控","slug":"监控","link":"/tags/%E7%9B%91%E6%8E%A7/"},{"name":"可视化","slug":"可视化","link":"/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"持续集成","slug":"持续集成","link":"/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"}],"categories":[{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Redis","slug":"中间件/Redis","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Redis/"},{"name":"K8S","slug":"中间件/K8S","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/K8S/"},{"name":"编程语言","slug":"编程语言","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"nginx","slug":"中间件/nginx","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/nginx/"},{"name":"Prometheus","slug":"中间件/Prometheus","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Prometheus/"},{"name":"Golang","slug":"编程语言/Golang","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/"},{"name":"Python","slug":"编程语言/Python","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/"},{"name":"Jenkins","slug":"中间件/Jenkins","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Jenkins/"}]}