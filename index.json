[{"categories":["K8S"],"content":"本文主要介绍Kubernetes网络模型。 Kubernetes 用来在集群上运行分布式系统。分布式系统的本质使得网络组件在 Kubernetes 中是至关重要也不可或缺的。理解 Kubernetes 的网络模型可以帮助你更好的在 Kubernetes 上运行、监控、诊断你的应用程序。 网络是一个很宽泛的领域，其中有许多成熟的技术。对于不熟悉网络整体背景的人而言，要将各种新的概念、旧的概念放到一起来理解（例如，网络名称空间、虚拟网卡、IP forwarding、网络地址转换等），并融汇贯通，是一个非常困难的事情。本文将尝试揭开 Kubernetes 网络的面纱，并讨论 Kubernetes 相关的网络技术，以及这些技术是如何支持 Kubernetes 网络模型的。 文章有点长，分成主要的几个部分： 首先讨论一些 Kubernetes 基础的术语，确保大家对关键措辞的理解是一致的 然后讨论 Kubernetes 网络模型，及其设计和实现 主要的内容是：通过不同的 use case 深入探讨 Kubernetes 中网络流量是如何路由的 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:0:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Kubernetes基本概念 Kubernetes 基于少数几个核心概念，不断完善，提供了非常丰富和实用的功能。本章节罗列了这些核心概念，并简要的做了概述，以便更好地支持后面的讨论。熟悉 Kubernetes 的读者可跳过这个章节。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:1:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Kubernetes API Server 操作 Kubernetes 的方式，是调用 Kubernetes API Server（kube-apiserver）的 API 接口。kubectl、kubernetes dashboard、kuboard 都是通过调用 kube-apiserver 的接口实现对 kubernetes 的管理。API server 最终将集群状态的数据存储在 etcd (opens new window)中。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:1:1","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"控制器Controller 控制器（Controller）是 Kubernetes 中最核心的抽象概念。在用户通过 kube-apiserver 声明了期望的状态以后，控制器通过不断监控 apiserver 中的当前状态，并对当前状态与期望状态之间的差异做出反应，以确保集群的当前状态不断地接近用户声明的期望状态。这个过程实现在一个循环中，参考如下伪代码： while true: X = currentState() Y = desiredState() if X == Y: return # Do nothing else: do(tasks to get to Y) 例如，当你通过 API Server 创建一个新的 Pod 对象时，Kubernetes调度器（是一个控制器）注意到此变化，并做出将该 Pod 运行在集群中哪个节点的决定。然后，通过 API Server 修改 Pod 对象的状态。此时，对应节点上的kubelet（是一个控制器）注意到此变化，并将在其所在节点运行该 Pod，设置需要的网络，使 Pod 在集群内可以访问。此处，两个控制器针对不同的状态变化做出反应，以使集群的当前状态与用户指定的期望状态匹配。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:1:2","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"容器组Pod Pod 是 Kubernetes 中的最小可部署单元。一个 Pod 代表了集群中运行的一个工作负载，可以包括一个或多个 docker 容器、挂载需要的存储，并拥有唯一的 IP 地址。Pod 中的多个容器将始终在同一个节点上运行。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:1:3","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"节点Node 节点是Kubernetes集群中的一台机器，可以是物理机，也可以是虚拟机。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:1:4","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Kubernetes网络模型 关于 Pod 如何接入网络这件事情，Kubernetes 做出了明确的选择。具体来说，Kubernetes 要求所有的网络插件实现必须满足如下要求： 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射（network address translation） 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射 Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个 在这些限制条件下，需要解决如下四种完全不同的网络使用场景的问题： Container-to-Container 的网络 Pod-to-Pod 的网络 Pod-to-Service 的网络 Internet-to-Service 的网络 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:2:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Container-to-Container的网络 通常，我们认为虚拟机中的网络通信是直接使用以太网设备进行的，如下图所示： 实际情况比这个示意图更加复杂一些。Linux系统中，每一个进程都在一个 network namespace (opens new window)中进行通信，network namespace 提供了一个逻辑上的网络堆栈（包含自己的路由、防火墙规则、网络设备）。换句话说，network namespace 为其中的所有进程提供了一个全新的网络堆栈。 Linux 用户可以使用 ip 命令创建 network namespace。例如，下面的命令创建了一个新的 network namespace 名称为 ns1： $ ip netns add ns1 当创建 network namespace 时，同时将在 /var/run/netns 下创建一个挂载点（mount point）用于存储该 namespace 的信息。 执行 ls /var/run/netns 命令，或执行 ip 命令，可以查看所有的 network namespace： $ ls /var/run/netns ns1 $ ip netns ns1 默认情况下，Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络，如下图所示： 在 Kubernetes 中，Pod 是一组 docker 容器的集合，这一组 docker 容器将共享一个 network namespace。Pod 中所有的容器都： 使用该 network namespace 提供的同一个 IP 地址以及同一个端口空间 可以通过 localhost 直接与同一个 Pod 中的另一个容器通信 Kubernetes 为每一个 Pod 都创建了一个 network namespace。具体做法是，把一个 Docker 容器当做 “Pod Container” 用来获取 network namespace，在创建 Pod 中新的容器时，都使用 docker run 的 --network:container 功能来加入该 network namespace，参考 docker run reference (opens new window)。如下图所示，每一个 Pod 都包含了多个 docker 容器（ctr*），这些容器都在同一个共享的 network namespace 中： 此外，Pod 中可以定义数据卷，Pod 中的容器都可以共享这些数据卷，并通过挂载点挂载到容器内部不同的路径，具体请参考 存储卷 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:3:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Pod-to-Pod的网络 在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。本章节可以帮助我们理解 Kubernetes 是如何在 Pod-to-Pod 通信中使用真实 IP 的，不管两个 Pod 是在同一个节点上，还是集群中的不同节点上。我们将首先讨论通信中的两个 Pod 在同一个节点上的情况，以避免引入跨节点网络的复杂性。 从 Pod 的视角来看，Pod 是在其自身所在的 network namespace 与同节点上另外一个 network namespace 进程通信。在Linux上，不同的 network namespace 可以通过 Virtual Ethernet Device (opens new window)或 veth pair (两块跨多个名称空间的虚拟网卡)进行通信。为连接 pod 的 network namespace，可以将 veth pair 的一段指定到 root network namespace，另一端指定到 Pod 的 network namespace。每一组 veth pair 类似于一条网线，连接两端，并可以使流量通过。节点上有多少个 Pod，就会设置多少组 veth pair。下图展示了 veth pair 连接 Pod 到 root namespace 的情况： 此时，我们的 Pod 都有了自己的 network namespace，从 Pod 的角度来看，他们都有自己的以太网卡以及 IP 地址，并且都连接到了节点的 root network namespace。为了让 Pod 可以互相通过 root network namespace 通信，我们将使用 network bridge（网桥）。 Linux Ethernet bridge 是一个虚拟的 Layer 2 网络设备，可用来连接两个或多个网段（network segment）。网桥的工作原理是，在源于目标之间维护一个转发表（forwarding table），通过检查通过网桥的数据包的目标地址（destination）和该转发表来决定是否将数据包转发到与网桥相连的另一个网段。桥接代码通过网络中具备唯一性的网卡MAC地址来判断是否桥接或丢弃数据。 网桥实现了 ARP (opens new window)协议，以发现链路层与 IP 地址绑定的 MAC 地址。当网桥收到数据帧时，网桥将该数据帧广播到所有连接的设备上（除了发送者以外），对该数据帧做出相应的设备被记录到一个查找表中（lookup table）。后续网桥再收到发向同一个 IP 地址的流量时，将使用查找表（lookup table）来找到对应的 MAC 地址，并转发数据包。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:4:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"数据包的传递：Pod-to-Pod，同节点 在 network namespace 将每一个 Pod 隔离到各自的网络堆栈的情况下，虚拟以太网设备（virtual Ethernet device）将每一个 namespace 连接到 root namespace，网桥将 namespace 又连接到一起，此时，Pod 可以向同一节点上的另一个 Pod 发送网络报文了。下图演示了同节点上，网络报文从一个Pod传递到另一个Pod的情况。 Pod1 发送一个数据包到其自己的默认以太网设备 eth0。 对 Pod1 来说，eth0 通过虚拟以太网设备（veth0）连接到 root namespace 网桥 cbr0 中为 veth0 配置了一个网段。一旦数据包到达网桥，网桥使用ARP (opens new window)协议解析出其正确的目标网段 veth1 网桥 cbr0 将数据包发送到 veth1 数据包到达 veth1 时，被直接转发到 Pod2 的 network namespace 中的 eth0 网络设备。 在整个数据包传递过程中，每一个 Pod 都只和 localhost 上的 eth0 通信，且数包被路由到正确的 Pod 上。与开发人员正常使用网络的习惯没有差异。 Kubernetes 的网络模型规定，在跨节点的情况下 Pod 也必须可以通过 IP 地址访问。也就是说，Pod 的 IP 地址必须始终对集群中其他 Pod 可见；且从 Pod 内部和从 Pod 外部来看，Pod 的IP地址都是相同的。接下来我们讨论跨节点情况下，网络数据包如何传递。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:4:1","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"数据包的传递：Pod-to-Pod，跨节点 在了解了如何在同节点上 Pod 之间传递数据包之后，我们接下来看看如何在跨节点的 Pod 之间传递数据包。Kubernetes 网络模型要求 Pod 的 IP 在整个网络中都可访问，但是并不指定如何实现这一点。实际上，这是所使用网络插件相关的，但是，仍然有一些模式已经被确立了。 通常，集群中每个节点都被分配了一个 CIDR 网段，指定了该节点上的 Pod 可用的 IP 地址段。一旦发送到该 CIDR 网段的流量到达节点，就由节点负责将流量继续转发给对应的 Pod。下图展示了两个节点之间的数据报文传递过程。 图中，目标 Pod（以绿色高亮）与源 Pod（以蓝色高亮）在不同的节点上，数据包传递过程如下： 数据包从 Pod1 的网络设备 eth0，该设备通过 veth0 连接到 root namespace 数据包到达 root namespace 中的网桥 cbr0 网桥上执行 ARP 将会失败，因为与网桥连接的所有设备中，没有与该数据包匹配的 MAC 地址。一旦 ARP 失败，网桥会将数据包发送到默认路由（root namespace 中的 eth0 设备）。此时，数据包离开节点进入网络 假设网络可以根据各节点的CIDR网段，将数据包路由到正确的节点 数据包进入目标节点的 root namespace（VM2 上的 eth0）后，通过网桥路由到正确的虚拟网络设备（veth1） 最终，数据包通过 veth1 发送到对应 Pod 的 eth0，完成了数据包传递的过程 通常来说，每个节点知道如何将数据包分发到运行在该节点上的 Pod。一旦一个数据包到达目标节点，数据包的传递方式与同节点上不同Pod之间数据包传递的方式就是一样的了。 此处，我们直接跳过了如何配置网络，以使得数据包可以从一个节点路由到匹配的节点。这些是与具体的网络插件实现相关的，如果感兴趣，可以深入查看某一个网络插件的具体实现。例如，AWS上，亚马逊提供了一个 Container Network Interface(CNI) plugin (opens new window)使得 Kubernetes 可以在 Amazon VPC 上执行节点到节点的网络通信。 Container Network Interface(CNI) plugin 提供了一组通用 API 用来连接容器与外部网络。具体到容器化应用开发者来说，只需要了解在整个集群中，可以通过 Pod 的 IP 地址直接访问 Pod；网络插件是如何做到跨节点的数据包传递这件事情对容器化应用来说是透明的。AWS 的 CNI 插件通过利用 AWS 已有的 VPC、IAM、Security Group 等功能提供了一个满足 Kubernetes 网络模型要求的，且安全可管理的网络环境。 在 EC2（AWS 的虚拟机服务） 中，每一个实例都绑定到一个 elastic network interface （ENI）并且 VPC 中所有的 ENI 都是可连通的。默认情况下，每一个 EC2 实例都有一个唯一的 ENI，但是可以随时为 EC2 实例创建多个 ENI。AWS 的 kubernetes CNI plugin 利用了这个特点，并为节点上的每一个 Pod 都创建了一个新的 ENI。由于在 AWS 的基础设施中， VPC 当中的 ENI 已经相互连接了，这就使得每一个 Pod 的 IP 地址天然就可在 VPC 内直接访问。当 CNI 插件安装到集群上是，每一个节点（EC2实例）创建多个 elastic network interface 并且为其申请到 IP 地址，在节点上形成一个 CIDR 网段。当 Pod 被部署时，kubernetes 集群上以 DaemonSet 形式部署的一段程序将接收到该节点上 kubelet 发出的添加 Pod 到 网络的请求。这段程序将从节点的可用 ENI 池中找出一个可用的 IP 地址，并将 ENI 及 IP 地址分配给 Pod，具体做法是按照 数据包的传递：Pod-to-Pod，同节点 中描述的方式在 Linux 内核中连接虚拟网络设备和网桥。此时，Pod 可以被集群内任意节点访问了。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:4:2","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Pod-to-Service的网络 我们已经了解了如何在 Pod 的 IP 地址之间传递数据包。然而，Pod 的 IP 地址并非是固定不变的，随着 Pod 的重新调度（例如水平伸缩、应用程序崩溃、节点重启等），Pod 的 IP 地址将会出现又消失。此时，Pod 的客户端无法得知该访问哪一个 IP 地址。Kubernetes 中，Service 的概念用于解决此问题。 一个 Kubernetes Service 管理了一组 Pod 的状态，可以追踪一组 Pod 的 IP 地址的动态变化过程。一个 Service 拥有一个 IP 地址，并且充当了一组 Pod 的 IP 地址的“虚拟 IP 地址”。任何发送到 Service 的 IP 地址的数据包将被负载均衡到该 Service 对应的 Pod 上。在此情况下，Service 关联的 Pod 可以随时间动态变化，客户端只需要知道 Service 的 IP 地址即可（该地址不会发生变化）。 从效果上来说，Kubernetes 自动为 Service 创建和维护了集群内部的分布式负载均衡，可以将发送到 Service IP 地址的数据包分发到 Service 对应的健康的 Pod 上。接下来我们讨论一下这是怎么做到的。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"netfilter and iptables Kubernetes 利用 Linux 内建的网络框架 - netfilter 来实现负载均衡。Netfilter 是由 Linux 提供的一个框架，可以通过自定义 handler 的方式来实现多种网络相关的操作。Netfilter 提供了许多用于数据包过滤、网络地址转换、端口转换的功能，通过这些功能，自定义的 handler 可以在网络上转发数据包、禁止数据包发送到敏感的地址，等。 iptables 是一个 user-space 应用程序，可以提供基于决策表的规则系统，以使用 netfilter 操作或转换数据包。在 Kubernetes 中，kube-proxy 控制器监听 apiserver 中的变化，并配置 iptables 规则。当 Service 或 Pod 发生变化时（例如 Service 被分配了 IP 地址，或者新的 Pod 被关联到 Service），kube-proxy 控制器将更新 iptables 规则，以便将发送到 Service 的数据包正确地路由到其后端 Pod 上。iptables 规则将监听所有发向 Service 的虚拟 IP 的数据包，并将这些数据包转发到该Service 对应的一个随机的可用 Pod 的 IP 地址，同时 iptables 规则将修改数据包的目标 IP 地址（从 Service 的 IP 地址修改为选中的 Pod 的 IP 地址）。当 Pod 被创建或者被终止时，iptables 的规则也被对应的修改。换句话说，iptables 承担了从 Service IP 地址到实际 Pod IP 地址的负载均衡的工作。 在返回数据包的路径上，数据包从目标 Pod 发出，此时，iptables 规则又将数据包的 IP 头从 Pod 的 IP 地址替换为 Service 的 IP 地址。从请求的发起方来看，就好像始终只是在和 Service 的 IP 地址通信一样。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:1","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"IPVS Kubernetes v1.11 开始，提供了另一个选择用来实现集群内部的负载均衡：IPVS。 IPVS（IP Virtual Server）也是基于 netfilter 构建的，在 Linux 内核中实现了传输层的负载均衡。IPVS 被合并到 LVS（Linux Virtual Server）当中，充当一组服务器的负载均衡器。IPVS 可以转发 TCP / UDP 请求到实际的服务器上，使得一组实际的服务器看起来像是只通过一个单一 IP 地址访问的服务一样。IPVS 的这个特点天然适合与用在 Kubernetes Service 的这个场景下。 当声明一个 Kubernetes Service 时，你可以指定是使用 iptables 还是 IPVS 来提供集群内的负载均衡工鞥呢。IPVS 是转为负载均衡设计的，并且使用更加有效率的数据结构（hash tables），相较于 iptables，可以支持更大数量的网络规模。当创建使用 IPVS 形式的 Service 时，Kubernetes 执行了如下三个操作： 在节点上创建一个 dummy IPVS interface 将 Service 的 IP 地址绑定到该 dummy IPVS interface 为每一个 Service IP 地址创建 IPVS 服务器 将来，IPVS 有可能成为 kubernetes 中默认的集群内负载均衡方式。这个改变将只影响到集群内的负载均衡，本文后续讨论将以 iptables 为例子，所有讨论对 IPVS 是同样适用的。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:2","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"数据包的传递：Pod-to-Service 在 Pod 和 Service 之间路由数据包时，数据包的发起和以前一样： 数据包首先通过 Pod 的 eth0 网卡发出 数据包经过虚拟网卡 veth0 到达网桥 cbr0 网桥上的 APR 协议查找不到该 Service，所以数据包被发送到 root namespace 中的默认路由 - eth0 此时，在数据包被 eth0 接受之前，数据包将通过 iptables 过滤。iptables 使用其规则（由 kube-proxy 根据 Service、Pod 的变化在节点上创建的 iptables 规则）重写数据包的目标地址（从 Service 的 IP 地址修改为某一个具体 Pod 的 IP 地址） 数据包现在的目标地址是 Pod 4，而不是 Service 的虚拟 IP 地址。iptables 使用 Linux 内核的 conntrack 工具包来记录具体选择了哪一个 Pod，以便可以将未来的数据包路由到同一个 Pod。简而言之，iptables 直接在节点上完成了集群内负载均衡的功能。数据包后续如何发送到 Pod 上，其路由方式与 Pod-to-Pod的网络 中的描述相同。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:3","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"数据包的传递：Service-to-Pod 接收到此请求的 Pod 将会发送返回数据包，其中标记源 IP 为接收请求 Pod 自己的 IP，目标 IP 为最初发送对应请求的 Pod 的 IP 当数据包进入节点后，数据包将经过 iptables 的过滤，此时记录在 conntrack 中的信息将被用来修改数据包的源地址（从接收请求的 Pod 的 IP 地址修改为 Service 的 IP 地址） 然后，数据包将通过网桥、以及虚拟网卡 veth0 最终到达 Pod 的网卡 eth0 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:4","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"使用DNS Kubernetes 也可以使用 DNS，以避免将 Service 的 cluster IP 地址硬编码到应用程序当中。Kubernetes DNS 是 Kubernetes 上运行的一个普通的 Service。每一个节点上的 kubelet 都使用该 DNS Service 来执行 DNS 名称的解析。集群中每一个 Service（包括 DNS Service 自己）都被分配了一个 DNS 名称。DNS 记录将 DNS 名称解析到 Service 的 ClusterIP 或者 Pod 的 IP 地址。SRV 记录 用来指定 Service 的已命名端口。 DNS Pod 由三个不同的容器组成： kubedns：观察 Kubernetes master 上 Service 和 Endpoints 的变化，并维护内存中的 DNS 查找表 dnsmasq：添加 DNS 缓存，以提高性能 sidecar：提供一个健康检查端点，可以检查 dnsmasq 和 kubedns 的健康状态 DNS Pod 被暴露为 Kubernetes 中的一个 Service，该 Service 及其 ClusterIP 在每一个容器启动时都被传递到容器中（环境变量及 /etc/resolves），因此，每一个容器都可以正确的解析 DNS。DNS 条目最终由 kubedns 解析，kubedns 将 DNS 的所有信息都维护在内存中。etcd 中存储了集群的所有状态，kubedns 在必要的时候将 etcd 中的 key-value 信息转化为 DNS 条目信息，以重建内存中的 DNS 查找表。 CoreDNS 的工作方式与 kubedns 类似，但是通过插件化的架构构建，因而灵活性更强。自 Kubernetes v1.11 开始，CoreDNS 是 Kubernetes 中默认的 DNS 实现。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:5:5","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"Internet-to-Service的网络 前面我们已经了解了 Kubernetes 集群内部的网络路由。下面，我们来探讨一下如何将 Service 暴露到集群外部： 从集群内部访问互联网 从互联网访问集群内部 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:6:0","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"出方向 - 从集群内部访问互联网 将网络流量从集群内的一个节点路由到公共网络是与具体网络以及实际网络配置紧密相关的。为了更加具体地讨论此问题，本文将使用 AWS VPC 来讨论其中的具体问题。 在 AWS，Kubernetes 集群在 VPC 内运行，在此处，每一个节点都被分配了一个内网地址（private IP address）可以从 Kubernetes 集群内部访问。为了使访问外部网络，通常会在 VPC 中添加互联网网关（Internet Gateway），以实现如下两个目的： 作为 VPC 路由表中访问外网的目标地址 提供网络地址转换（NAT Network Address Translation），将节点的内网地址映射到一个外网地址，以使外网可以访问内网上的节点 在有互联网网关（Internet Gateway）的情况下，虚拟机可以任意访问互联网。但是，存在一个小问题：Pod 有自己的 IP 地址，且该 IP 地址与其所在节点的 IP 地址不一样，并且，互联网网关上的 NAT 地址映射只能够转换节点（虚拟机）的 IP 地址，因为网关不知道每个节点（虚拟机）上运行了哪些 Pod （互联网网关不知道 Pod 的存在）。接下来，我们了解一下 Kubernetes 是如何使用 iptables 解决此问题的。 数据包的传递：Node-to-Internet 下图中： 数据包从 Pod 的 network namespace 发出 通过 veth0 到达虚拟机的 root network namespace 由于网桥上找不到数据包目标地址对应的网段，数据包将被网桥转发到 root network namespace 的网卡 eth0。在数据包到达 eth0 之前，iptables 将过滤该数据包。 在此处，数据包的源地址是一个 Pod，如果仍然使用此源地址，互联网网关将拒绝此数据包，因为其 NAT 只能识别与节点（虚拟机）相连的 IP 地址。因此，需要 iptables 执行源地址转换（source NAT），这样子，对互联网网关来说，该数据包就是从节点（虚拟机）发出的，而不是从 Pod 发出的 数据包从节点（虚拟机）发送到互联网网关 互联网网关再次执行源地址转换（source NAT），将数据包的源地址从节点（虚拟机）的内网地址修改为网关的外网地址，最终数据包被发送到互联网 在回路径上，数据包沿着相同的路径反向传递，源地址转换（source NAT）在对应的层级上被逆向执行。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:6:1","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["K8S"],"content":"入方向 - 从互联网访问Kubernetes 入方向访问（从互联网访问Kubernetes集群）是一个非常棘手的问题。该问题同样跟具体的网络紧密相关，通常来说，入方向访问在不同的网络堆栈上有两个解决方案： Service LoadBalancer Ingress Controller Layer 4：LoadBalancer 当创建 Kubernetes Service 时，可以指定其类型为 LoadBalancer。 LoadBalancer 的实现由 cloud controller (opens new window)提供，cloud controller 可以调用云供应商 IaaS 层的接口，为 Kubernetes Service 创建负载均衡器（如果您自建 Kubernetes 集群，可以使用 NodePort 类型的 Service，并手动创建负载均衡器）。用户可以将请求发送到负载均衡器来访问 Kubernetes 中的 Service。 在 AWS，负载均衡器可以将网络流量分发到其目标服务器组（即 Kubernetes 集群中的所有节点）。一旦数据包到达节点，Service 的 iptables 规则将确保其被转发到 Service 的一个后端 Pod。 数据包的传递：LoadBalancer-to-Service 接下来了解一下 Layer 4 的入方向访问具体是如何做到的： Loadbalancer 类型的 Service 创建后，cloud controller 将为其创建一个负载均衡器 负载均衡器只能直接和节点（虚拟机沟通），不知道 Pod 的存在，当数据包从请求方（互联网）到达 LoadBalancer 之后，将被分发到集群的节点上 节点上的 iptables 规则将数据包转发到合适的 Pod 上 （同 数据包的传递：Service-to-Pod） 从 Pod 到请求方的相应数据包将包含 Pod 的 IP 地址，但是请求方需要的是负载均衡器的 IP 地址。iptables 和 conntrack 被用来重写返回路径上的正确的 IP 地址。 下图描述了一个负载均衡器和三个集群节点： 请求数据包从互联网发送到负载均衡器 负载均衡器将数据包随机分发到其中的一个节点（虚拟机），此处，我们假设数据包被分发到了一个没有对应 Pod 的节点（VM2）上 在 VM2 节点上，kube-proxy 在节点上安装的 iptables 规则会将该数据包的目标地址判定到对应的 Pod 上（集群内负载均衡将生效） iptables 完成 NAT 映射，并将数据包转发到目标 Pod Layer 7：Ingress控制器 译者注 本章节讲述的 Ingress 控制器实现方式是特定于 AWS 的，与 nginx ingress controller 的具体做法有所不同 Layer 7 网络入方向访问在网络堆栈的 HTTP/HTTPS 协议层面工作，并且依赖于 KUbernetes Service。要实现 Layer 7 网络入方向访问，首先需要将 Service 指定为 NodtePort 类型，此时 Kubernetes master 将会为该 Service 分配一个 节点端口，每一个节点上的 iptables 都会将此端口上的请求转发到 Service 的后端 Pod 上。此时，Service-to-Pod 的路由与 数据包的传递：Service-to-Pod 的描述相同。 接下来，创建一个 Kubernetes Ingress 对象可以将该 Service 发布到互联网。Ingress 是一个高度抽象的 HTTP 负载均衡器，可以将 HTTP 请求映射到 Kubernetes Service。在不同的 Kubernetes 集群中，Ingress 的具体实现可能是不一样的。与 Layer 4 的网络负载均衡器相似，HTTP 负载均衡器只理解节点的 IP 地址（而不是 Pod 的 IP 地址），因此，也同样利用了集群内部通过 iptables 实现的负载均衡特性。 在 AWS 中，ALB Ingress 控制器使用 Amazon 的 Layer 7 Application Load Balancer实现了 Kubernetes Ingress 的功能。下图展示了 AWS 上 Ingress 控制器的细节，也展示了网络请求是如何从 ALB 路由到 Kubernetes 集群的。 ALB Ingress Controller 创建后，将监听 Kubernetes API 上关于 Ingress 的事件。当发现匹配的 Ingress 对象时，Ingress Controller 开始创建 AWS 资源 AWS 使用 Application Load Balancer（ALB）来满足 Ingress 对象的要求，并使用 Target Group 将请求路由到目标节点 ALB Ingress Controller 为 Kubernetes Ingress 对象中用到的每一个 Kubernetes Service 创建一个 AWS Target Group Listener 是一个 ALB 进程，由 ALB Ingress Controller 根据 Ingress 的注解（annotations）创建，监听 ALB 上指定的协议和端口，并接收外部的请求 ALB Ingress Controller 还根据 Kubernetes Ingress 中的路径定义，创建了 Target Group Rule，确保指定路径上的请求被路由到合适的 Kubernetes Service 数据包的传递：Ingress-to-Service Ingress-to-Service 的数据包传递与 LoadBalancer-to-Service 的数据包传递非常相似。核心差别是： Ingress 能够解析 URL 路径（可基于路径进行路由） Ingress 连接到 Service 的 NodePort 下图展示了 Ingress-to-Service 的数据包传递过程。 创建 Ingress 之后，cloud controller 将会为其创建一个新的 Ingress Load Balancer 由于 Load Balancer 并不知道 Pod 的 IP 地址，当路由到达 Ingress Load Balancer 之后，会被转发到集群中的节点上（Service的节点端口） 节点上的 iptables 规则将数据包转发到合适的 Pod Pod 接收到数据包 从 Pod 返回的响应数据包将包含 Pod 的 IP 地址，但是请求客户端需要的是 Ingress Load Balancer 的 IP 地址。iptables 和 conntrack 被用来重写返回路径上的 IP 地址。 ","date":"2022-09-08","objectID":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/:6:2","tags":["k8s"],"title":"Kubernetes网络模型","uri":"/kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"categories":["中间件"],"content":"使用trojan-go进行搭建。 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:0:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"什么是Trojan-go 使用Go实现的完整Trojan代理，与Trojan协议以及Trojan版本的配置文件格式兼容。安全，高效，轻巧，易用。详见官网 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:1:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名准备 域名申请可以去腾讯云或者阿里云等国内大厂申请（缺点就是需要花钱），这里我是去的免费域名商freenom申请的http://www.freenom.com/zh/index.html，不过访问这个地址很慢，所以可能需要提前出去。 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:2:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名申请 首先注册账号，基本信息里实测国家选择美国以外的地区可能无法注册成功，所以这里我选择的洛杉矶 输入希望申请的域名，点击检查可用性 选择心仪的域名后缀，点击现在获取 域名时长我们一般选择最长时间1年，然后将VPS的IP填入解析地址（也可以现在不填，申请完成后在My Domains里面进行填写） ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:2:1","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名测试 默认的DNS生效时间比较长，大概半小时左右。本地能ping通域名，返回的地址为希望的IP，表示域名申请成功 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:2:2","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"证书申请 一键搭建脚本可以自动申请证书（有效期3个月），证书申请步骤可以跳过 证书申请也是本着不花钱的原则，选择https://freessl.cn/ 因为只有亚洲诚信可以支持双域名，而且有效期可以1年，所以选择亚洲诚信，点击创建免费的SSL证书 填入个人邮箱，然后点击创建 登录FreeSSL的账号，安装亚洲诚信的KeyManager 点击继续后，会自动带起KeyManager，生成秘钥 回到浏览器，在浏览器上会提示DNS验证 这时我们回到freenom点击My Domains，点击Manage Domain，选择Manage Freenom DNS 将证书生成的信息填入DNS的解析里面，Type选择TXT，然后点击保存 回到freessl页面，点击配置完成（可能有30分钟左右延迟），检测一下，当检测通过，点击验证即可签发证书，证书签发以后，可以使用https访问一下自己的域名，可以看到证书是有效的 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:3:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"Torjan-go搭建 此处我使用的是Centos7的系统 首先安装依赖包 yum update -y \u0026\u0026 yum install -y curl ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:4:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安装BBR加速 wget -N --no-check-certificate \"https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh\" \u0026\u0026 chmod +x tcp.sh \u0026\u0026 ./tcp.sh 选择需要的BBR加速内核，我的服务器默认已经安装了BBR内核，因此我直接选择4开启 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:4:1","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"一键安装trojan-go #安装/更新 source \u003c(curl -sL https://git.io/trojan-install) 根据提示安装完毕后，可以使用trojan执行管理管理 或者通过域名在web端进行管理，可以查看当前负载以及添加和用户管理（admin密码可以通过上一图的web管理进行重置） #卸载 source \u003c(curl -sL https://git.io/trojan-install) --remove ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:4:2","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"CND加速 此步骤目的为降低网络延迟，不配置也不影响使用，不过墙裂推荐，速度快了不止一倍 如果要使用CDN加速，需要将trojan切换为trojan-go（脚本默认搭建为trojan），Trojan协议本身不带加密，安全性依赖外层的TLS。但流量一旦经过CDN，TLS对CDN是透明的。其服务提供者可以对TLS的明文内容进行审查。如果你使用的是不可信任的CDN（任何在中国大陆注册备案的CDN服务均应被视为不可信任），请务必开启Shadowsocks AEAD对Webosocket流量进行加密，以避免遭到识别和审查。 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:5:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"cloudflare创建反代理加速 注册登录cloudflare，根据提示拿到Cloudflare提供的DNS服务器 回到域名提供商，将域名的DNS解析到Cloudflare 配置完成后，可以在cloudflare首页看到域名是有效的，此时如果ping域名的话，是能看到域名解析的ip已经不是vps的ip了 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:5:1","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"开启CDN，并设置CDN 点击DNS，代理状态为黄色，表示代理成功 点击页面中的 SSL/TLS 进入如下界面并设置如图所示：（重要） ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:5:2","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"websocket开启 若是需要Trojan套用CDN，也就是必须开启 websocket 首先找到trojan-go的配置文件，然后进行备份 在配置文件下面添加这2段，更多配置详情见trojan-go官方文档 \"websocket\": { \"enabled\": true, \"path\": \"/DFE4545DFDED/\", \"host\": \"域名\" } host是主机名，一般填写域名。客户端host是可选的，填写你的域名。如果留空，将会使用remote_addr填充。 path指的是websocket所在的URL路径，必须以斜杠(\"/\")开始。路径并无特别要求，满足URL基本格式即可，但要保证客户端和服务端的path一致。path应当选择较长的字符串，以避免遭到GFW直接的主动探测。 客户端的host将包含在Websocket的握手HTTP请求中，发送给CDN服务器，必须有效；服务端和客户端path必须一致，否则Websocket握手无法进行。 \"mux\": { \"enabled\": true, \"concurrency\": 8, \"idle_timeout\": 60 } 启用多路复用不会增加你的链路速度（甚至会有所减少），而且可能会增加服务器和客户端的计算负担。可以粗略地理解为，多路复用牺牲网络吞吐和CPU功耗，换取更低的延迟。在高并发的情景下，如浏览含有大量图片的网页时，或者发送大量UDP请求时，可以提升使用体验。 concurrency是每个TLS连接最多可以承载的TCP连接数。这个数值越大，每个TLS连接被复用的程度就更高，握手导致的延迟越低。但服务器和客户端的计算负担也会越大，这有可能使你的网络吞吐量降低。如果你的线路的TLS握手极端缓慢，你可以将这个数值设置为-1，Trojan-Go将只进行一次TLS握手，只使用唯一的一条TLS连接进行传输。 idle_timeout指的是每个TLS连接空闲多长时间后关闭。设置超时时间，可能有助于减少不必要的长连接存活确认(Keep Alive)流量传输引发GFW的探测。你可以将这个数值设置为-1，TLS连接在空闲时将被立即关闭。 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:5:3","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"windows客户端配置 编辑连接，勾选Mux，Type选择ws，host和Path和服务端配置文件保持一致 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:5:4","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"windows配置 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"Qv2ray及插件安装 官网下载最新版本,以及trojan-go插件QvPlugin-Trojan-Go，插件放置在Qv2ray安装目录的plugins目录下，启动Qv2ray的时候会自动加载插件 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:1","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"trojan-go核心下载 官网下载最新版本，为了方便管理，在Qv2ray安装目录下创建了个trojan-go目录，将文件放置在此处 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:2","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"v2ray-core下载 官网下载最新版本，为了方便管理，在Qv2ray安装目录下创建了个core目录，将文件放置在此处 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:3","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"Qv2ray配置 插件配置 运行Qv2ray客户端，点击插件，选择插件QvPlugin-Trojan-Go，点击设定，点击Browser，选择上一步下载的trojan-go核心 v2ray-core配置 点击首选项，选择内核设置，选择上一步下载的v2ray-core 新建连接 点击新建，配置如图 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:4","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"验证 访问google，能正常访问，Qv2ray能看到访问的日志，搭建成功 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:6:5","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安卓配置 下载Trojan安卓客户端（一般选择*-universal-release.apk）https://github.com/trojan-gfw/igniter/releases，安装到手机即可。如果是使用trojan-go的建议使用trojan-go的安卓客户端，速度会比原版快很多。 ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:7:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"相关软件 如果软件下载不方便可以从百度云下载，上传的软件不会再更新了，如果能在gitbub下载，尽量去gitbub下载新版 链接: https://pan.baidu.com/s/1uKDzaBgt01iY4a15AQHwhg 提取码: tjhi ","date":"2021-01-23","objectID":"/trojan-go%E6%90%AD%E5%BB%BA/:8:0","tags":["torjan"],"title":"Trojan-go搭建","uri":"/trojan-go%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"使用trojan原版进行搭建。 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:0:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"什么是Trojan Trojan 是一个比较新的翻墙软件，它模仿了互联网上最常见的HTTPS协议，以诱骗 GFW 认为它就是 HTTPS，从而不被识别。所谓魔高一尺道高一丈，墙在不断往上砌，那工具也得跟着变了。Trojan 工作在 443 端口，并且处理来自外界的 HTTPS 请求，如果是合法的 Trojan 请求，那么为该请求提供服务，否则将该流量转交给 WEB 服务器 Nginx，由 Nginx 为其提供服务。基于这个工作过程可以知道，Trojan 的一切表现均与 Nginx 一致，不会引入额外特征，从而达到无法识别的效果。当然，为了防止恶意探测，我们需要将 80 端口的流量全部重定向到 443 端口，并且服务器只暴露 80 和 443 端口，这样可以使得服务器与常见的 WEB 服务器表现一致。 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:1:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名准备 域名申请可以去腾讯云或者阿里云等国内大厂申请（缺点就是需要花钱），这里我是去的免费域名商freenom申请的http://www.freenom.com/zh/index.html，不过访问这个地址很慢，所以可能需要提前出去。 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:2:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名申请 首先注册账号，基本信息里实测国家选择美国以外的地区可能无法注册成功，所以这里我选择的洛杉矶 输入希望申请的域名，点击检查可用性 选择心仪的域名后缀，点击现在获取 域名时长我们一般选择最长时间1年，然后将VPS的IP填入解析地址（也可以现在不填，申请完成后在My Domains里面进行填写） ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:2:1","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"域名测试 默认的DNS生效时间比较长，大概半小时左右。本地能ping通域名，返回的地址为希望的IP，表示域名申请成功 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:2:2","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"证书申请 证书申请也是本着不花钱的原则，选择https://freessl.cn/ 因为只有亚洲诚信可以支持双域名，而且有效期可以1年，所以选择亚洲诚信，点击创建免费的SSL证书 填入个人邮箱，然后点击创建 登录FreeSSL的账号，安装亚洲诚信的KeyManager 点击继续后，会自动带起KeyManager，生成秘钥 回到浏览器，在浏览器上会提示DNS验证 这时我们回到freenom点击My Domains，点击Manage Domain，选择Manage Freenom DNS 将证书生成的信息填入DNS的解析里面，Type选择TXT，然后点击保存 回到freessl页面，点击配置完成（可能有30分钟左右延迟），检测一下，当检测通过，点击验证即可签发证书，证书签发以后，可以使用https访问一下自己的域名，可以看到证书是有效的 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:3:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"Torjan搭建 此处我使用的是Centos7的系统 首先安装依赖包 yum update -y \u0026\u0026 yum install sudo newt curl -y \u0026\u0026 sudo -i ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:4:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安装BBR加速 wget -N --no-check-certificate \"https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh\" \u0026\u0026 chmod +x tcp.sh \u0026\u0026 ./tcp.sh 选择需要的BBR加速内核，我的服务器默认已经安装了BBR内核，因此我直接选择4开启 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:4:1","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安装Trojan 下载一键安装脚本 curl -O https://raw.githubusercontent.com/atrandys/trojan/master/trojan_mult.sh \u0026\u0026 chmod +x trojan_mult.sh \u0026\u0026 ./trojan_mult.sh 根据提示信息填入域名和密码即可搭建完成，搭建完成后访问域名会出现一个网站 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:4:2","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"windows配置 脚本执行完成后会在/usr/src/trojan-cli/trojan-cli.zip目录下生成带有配置文件的windows软件，下载到本地解压后双击trojan.exe直接运行即可，详细配置见config.json ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:5:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安装v2rayN 到https://github.com/2dust/v2rayN/releases下载v2rayN客户端，此处下载的是稳定版 解压后运行软件，点击服务器，添加Socks服务器，填入地址如图 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:5:1","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"验证 访问google，能正常访问，在开启的Trojan和v2rayN都能看到访问的日志，搭建成功 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:5:2","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["中间件"],"content":"安卓配置 下载Trojan安卓客户端（一般选择*-universal-release.apk）https://github.com/trojan-gfw/igniter/releases，安装到手机即可 ","date":"2021-01-17","objectID":"/trojan%E6%90%AD%E5%BB%BA/:6:0","tags":["torjan"],"title":"Trojan搭建","uri":"/trojan%E6%90%AD%E5%BB%BA/"},{"categories":["nginx"],"content":"系统设计时一般会预估负载，当系统暴露在公网中时，恶意攻击或正常突发流量等都可能导致系统被压垮，而限流就是保护措施之一。限流即控制流量，本文将记录 Nginx 的一种速率限流设置。 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:0","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"ngx_http_limit_req_module ngx_http_limit_req_module模块提供限制请求处理速率能力，使用了漏桶算法(leaky bucket)，即能够强行保证请求的实时处理速度不会超过设置的阈值。 算法思想是： 令牌以固定速率产生，并缓存到令牌桶中； 令牌桶放满时，多余的令牌被丢弃； 请求要消耗等比例的令牌才能被处理； 令牌不够时，请求被缓存。 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:1","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"配置示例 http { #表示设置一块10m的共享内存来保存键值得状态，键为$uri，平均处理的频率不超过每秒1次 limit_req_zone $uri zone=one:10m rate=1r/s; ... server { ... location /limittest { #使用共享内存one，同时允许超过频率限制的请求数不多于5个,nodelay表示不希望超过的请求被延迟 limit_req zone=one burst=5 nodelay; #如果超出限制，返回429的返回码 limit_req_status 429; } ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:2","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"参数解析 设置共享内存区域和请求的最大突发大小。如果请求速率超过为某个区域配置的速率，则它们的处理会延迟（不配置nodelay参数情况下），从而使请求按指定速率处理。过多的请求被延迟，直到它们的数量超过最大突发大小，在这种情况下请求被终止并出现错误。默认情况下，最大突发大小等于零。 limit_req_zone key zone=zone:size rate=rate; key 若客户的请求匹配了key，则进入zone。可以是文本、变量，通常为Nginx变量。如$binary_remote_addr(客户的ip)，$uri(不带参数的请求地址)，$request_uri(带参数的请求地址)，$server_name(服务器名称)。支持组合使用，使用空格隔开。 zone 使用zone=one，指定此zone的名字为one。 size 在zone=name后面紧跟:size，指定此zone的内存大小。如zone=name:10m，代表name的共享内存大小为10m。通常情况下，1m可以保存16000个状态。 rate 使用rate=1r/s，限制平均1秒不超过1个请求。使用rate=1r/m，限制平均1分钟不超过1个请求。如果需要每秒小于一个请求的速率，则按每分钟请求（r/m）指定。 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:3","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"验证 编辑nginx配置文件nginx.conf,加入限流配置， 先执行nginx -t检查配置是否ok，如果返回success表示文件检查ok 然后执行nginx -s reload重启nginx，使配置生效 浏览器访问配置了限流的url进行验证，可以看到有429返回，表示限流配置生效 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:4","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"跳转自定义页面 nginx触发限流后返回的页面非常不友好，因此我们可以开启自定义error页面，使返回页面更加友好 下面这段代码可以配置在server级别，也可以配置在location级别，为了不影响别的文根，此处我们在location下配置 location /limittest { ... # 关键参数：这个变量开启后，我们才能自定义错误页面，当后端返回429，nginx拦截错误定义错误页面 fastcgi_intercept_errors on; error_page 429 /429.html; #error_page 429 = http://www.test.com/429.html; 也可以重定向到另一个url ... } 注意:如果nginx返回的是本地的html，则页面状态码和定义的相同。若是跳转到某个url，页面状态码为302。 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:5","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["nginx"],"content":"nginx常见内置变量 $args #这个变量等于请求行中的参数。 $content_length #请求头中的Content-length字段。 $content_type #请求头中的Content-Type字段。 $document_root #当前请求在root指令中指定的值。 $host #请求主机头字段，否则为服务器名称。 $http_user_agent #客户端agent信息 $http_cookie #客户端cookie信息 $limit_rate #这个变量可以限制连接速率。 $request_body_file #客户端请求主体信息的临时文件名。 $request_method #客户端请求的动作，通常为GET或POST。 $remote_addr #客户端的IP地址。 $remote_port #客户端的端口。 $remote_user #已经经过Auth Basic Module验证的用户名。 $request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 $query_string #与$args相同。 $scheme #HTTP方法（如http，https）。 $server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 $server_name #服务器名称。 $server_port #请求到达服务器的端口号。 $request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。 $uri #不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。 $document_uri #与$uri相同。 ","date":"2020-11-08","objectID":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/:0:6","tags":["nginx","限流"],"title":"nginx配置限流","uri":"/nginx%E9%85%8D%E7%BD%AE%E9%99%90%E6%B5%81/"},{"categories":["Golang"],"content":"记录一次使用goftp包文件上传异常的问题 ","date":"2020-10-19","objectID":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/:0:0","tags":["问题定位"],"title":"goftp 125 Data connection already open问题","uri":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/"},{"categories":["Golang"],"content":"问题现象 平常工作中，经常会涉及到ftp文件上传，所以使用第三方库github.com/dutchcoders/goftp，撸了个ftp上传脚本。之前使用都是正常上传，今天使用突然发现无法无法使用了，抛出的异常为125 Data connection already open; transfer starting，ftp服务器上文件名已经存在但是大小为0kb，因为编译成了exe格式，所以不涉及代码改动，顿时觉得比较奇怪，将代码又重新撸了一遍，未发现明显异常，报错依旧。 ","date":"2020-10-19","objectID":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/:1:0","tags":["问题定位"],"title":"goftp 125 Data connection already open问题","uri":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/"},{"categories":["Golang"],"content":"定位过程 在网上也没查到啥有用信息，只有一个ftp异常码的解释125 Data connection already open; transfer starting. 资料连接已经打开，开始传送资料. 。后查阅ftp相关资料，ftp在文件上传的时候，是通过发送STOR命令来实现的，然后期待ftp返回150，是一个成功的响应码，后续会上传成功。但是现在只返回了一个125。查看goftp源码ftp.go，当ftp连接上以后，会返回一个状态，如果不是以StatusFileOK开头，则return 继续查看status.go中定义的StatusFileOK，定义的值为150，所以返回125会抛出异常。 ","date":"2020-10-19","objectID":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/:2:0","tags":["问题定位"],"title":"goftp 125 Data connection already open问题","uri":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/"},{"categories":["Golang"],"content":"原因 经过一下午的排查，原因找到了，应该是国庆期间公司对ftp的服务器进行了升级。IIS7对响应码处理做了调整。http://support.microsoft.com/kb/2505047 ，IIS7前，对APPE，STOU ，STOR命令，passive模式响应125，active模式响应150。IIS7.5后不考虑连接模式了，只考虑当前的连接状态，未连接响应150，已连接响应125。 ","date":"2020-10-19","objectID":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/:3:0","tags":["问题定位"],"title":"goftp 125 Data connection already open问题","uri":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/"},{"categories":["Golang"],"content":"解决办法 使用了一个比较粗暴的办法，直接将源码status.go中的StatusFileOK=\"150\"改为StatusFileOK=\"125\"，重新编译脚本之后上传正常。因只在本地使用，未做兼容性测试。仅此记录 ","date":"2020-10-19","objectID":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/:4:0","tags":["问题定位"],"title":"goftp 125 Data connection already open问题","uri":"/goftp-125-data-connection-already-open%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"本文主要介绍k8s搭建，使用官方推荐工具kubeadm进行搭建，ETCD独立部署 ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:0:0","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"docker 安装 添加阿里镜像源地址 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装docker出现如下错误,当前服务器上安装的containerd.io版本为1.2.0-3.el7版本过低 [root@guohailan3 ~]# yum -y install docker-ce 上次元数据过期检查：0:00:07 前，执行于 2020年05月24日 星期日 03时42分48秒。 错误： 问题: package docker-ce-3:19.03.9-3.el7.x86_64 requires containerd.io \u003e= 1.2.2-3, but none of the providers can be installed - cannot install the best candidate for the job - package containerd.io-1.2.10-3.2.el7.x86_64 is excluded - package containerd.io-1.2.13-3.1.el7.x86_64 is excluded - package containerd.io-1.2.13-3.2.el7.x86_64 is excluded - package containerd.io-1.2.2-3.3.el7.x86_64 is excluded - package containerd.io-1.2.2-3.el7.x86_64 is excluded - package containerd.io-1.2.4-3.1.el7.x86_64 is excluded - package containerd.io-1.2.5-3.1.el7.x86_64 is excluded - package containerd.io-1.2.6-3.3.el7.x86_64 is excluded (尝试添加 '--skip-broken' 来跳过无法安装的软件包 或 '--nobest' 来不只使用最佳选择的软件包) 升级containerd.io，再次进行安装 wget https://download.docker.com/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm yum install containerd.io-1.2.6-3.3.el7.x86_64.rpm 配置开机启动和启动docker systemctl enable docker systemctl start docker 优化docker参数 官方文档表示，更改设置，令容器运行时和kubelet使用systemd作为cgroup驱动，以此使系统更为稳定。 tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://bk6kzfqm.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } EOF systemctl daemon-reload # 重新加载 systemctl restart docker # 重启docker ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:1:0","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"kubernet 安装 ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:2:0","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"安装kubectl #无法连接google，配置了阿里的源 cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 EOF yum install -y kubectl setenforce 0 #暂时关闭selinux sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux #Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动。 swapoff -a yum install -y kubeadm kubelet #安装kubeadm kubelet工具 ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:2:1","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"准备证书 证书需要在一台机器上生成，拷贝到别的机器上 准备证书管理工具 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x cfssl_linux-amd64 mv cfssl_linux-amd64 /usr/bin/cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x cfssljson_linux-amd64 mv cfssljson_linux-amd64 /usr/bin/cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 生成ETCD的TLS 秘钥和证书 # 创建 CA 配置文件 mkdir ssl \u0026\u0026 cd ssl cfssl print-defaults csr \u003e csr.json cat \u003e config.json \u003c\u003cEOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证； client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证； 创建ca证书请求 cat \u003e ca-csr.json \u003c\u003cEOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"GuangDong\", \"ST\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 创建CA证书和私钥 cfssl gencert -initca ca-csr.json | cfssljson -bare ca 创建 etcd 证书签名请求 cat \u003e etcd-csr.json \u003c\u003cEOF { \"CN\": \"etcd\", \"hosts\": [ \"192.168.31.48\", \"192.168.31.137\", \"192.168.31.226\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"GuangDong\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF hosts 字段指定授权使用该证书的 etcd 节点 IP； 每个节点IP 都要在里面 或者 每个机器申请一个对应IP的证书 生成 etcd 证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 将证书拷贝到所有服务器的指定目录 mkdir -p /etc/etcd/ssl cp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/ ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:2:2","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"安装ETCD 准备二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.4.9/etcd-v3.4.9-linux-amd64.tar.gz tar -vxf etcd-v3.4.9-linux-amd64.tar.gz cp etcd-v3.4.9-linux-amd64/etcd* /usr/bin/ chmod +x /usr/bin/etcd* 部署环境变量 export NODE_NAME=\"etcd-host1\" #当前部署的机器名称(随便定义，只要能区分不同机器即可) export NODE_IP=\"192.168.31.48\" # 当前部署的机器 IP export export NODE_IPS=\"192.168.31.48 192.168.31.137 192.168.31.226\" # etcd 集群所有机器 IP # etcd 集群间通信的IP和端口 export ETCD_NODES=\"etcd-host1=https://192.168.31.48:2380,etcd-host2=https://192.168.31.137:2380,etcd-host3=https://192.168.31.226:2380\" 创建etcd的systemd unit文件 mkdir -p /var/lib/etcd # 必须先创建工作目录 cat \u003e etcd.service \u003c\u003cEOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ ExecStart=/usr/bin/etcd \\\\ --name=${NODE_NAME} \\\\ --cert-file=/etc/etcd/ssl/etcd.pem \\\\ --key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\\\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --trusted-ca-file=/etc/etcd/ssl/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\\\ --initial-advertise-peer-urls=https://${NODE_IP}:2380 \\\\ --listen-peer-urls=https://${NODE_IP}:2380 \\\\ --listen-client-urls=https://${NODE_IP}:2379,http://127.0.0.1:2379 \\\\ --advertise-client-urls=https://${NODE_IP}:2379 \\\\ --initial-cluster-token=etcd-cluster-0 \\\\ --initial-cluster=${ETCD_NODES} \\\\ --initial-cluster-state=new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF –name：方便理解的节点名称，默认为 default，在集群中应该保持唯一，可以使用 hostname –data-dir：服务运行数据保存的路径，默认为 ${name}.etcd –snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘 –heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms –eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms –listen-peer-urls：和同伴通信的地址，比如 http://ip:2380，如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！ –listen-client-urls：对外提供服务的地址：比如 http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和 etcd 交互 –advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点 –initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点 –initial-cluster：集群中所有节点的信息，格式为 node1=http://ip1:2380,node2=http://ip2:2380,…。注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值 –initial-cluster-state：新建集群的时候，这个值为 new；假如已经存在的集群，这个值为 existing –initial-cluster-token：创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误 启动 etcd 服务 mv etcd.service /etc/systemd/system/ systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd 验证服务并配置etcdctl工具 etcdctl --endpoints=\"https://192.168.31.48:2379,https://192.168.31.137:2379,https://192.168.31.226:2379\" --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem endpoint health 为了后续操作方便可配置alias alias etcdctl='etcdctl --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=\"https://192.168.31.48:2379,https://192.168.31.137:2379,https://192.168.31.226:2379\"' ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:2:3","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"初始化master 创建master配置文件 cat \u003e /etc/kubernetes/config.yaml \u003c\u003cEOF apiVersion: kubeadm.k8s.io/v1beta1 # 国内不能访问 Google，修改为阿里云 imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration ### etcd 配置及秘钥 ### etcd: external: endpoints: - https://192.168.31.48:2379 - https://192.168.31.137:2379 - https://192.168.31.226:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcd ### calico 网络插件的子网 ### networking: podSubnet: \"10.0.0.0/16\" serviceSubnet: \"10.96.0.0/12\" ###k8s的版本### kubernetesVersion: 1.18.0 EOF 初始化master 可查看依赖的镜像版本 kubeadm config images list --kubernetes-version=v1.18.0 kubeadm init --config /etc/kubernetes/config.yaml #主节点进行初始化 出现如下提示表示配置成功 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.31.226:6443 --token v7b8zo.5hc1au8vl96xqyie \\ --discovery-token-ca-cert-hash sha256:908597386cd0311f24e2d7c95e40559e3137523078e69e1262643c6161abc10a 如果出现如下错误，需要按照上图提示，配置kube/config 多master初始化 如果希望部署多master需要用下面的配置文件，token和tokenTTL在初始化第一台master的时候注释掉 cat \u003e /etc/kubernetes/config.yaml \u003c\u003cEOF apiVersion: kubeadm.k8s.io/v1beta1 # 国内不能访问 Google，修改为阿里云 imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration ### etcd 配置及秘钥 ### etcd: external: endpoints: - https://192.168.31.48:2379 - https://192.168.31.137:2379 - https://192.168.31.226:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcd ### calico 网络插件的子网 ### networking: podSubnet: \"10.0.0.0/16\" serviceSubnet: \"10.96.0.0/12\" ###k8s的版本### kubernetesVersion: 1.18.0 ######多master配置####### #下面的token是master1节点初始化完成后，join得到的token值 token: \"1gddb4.cs1chtdrk5r9aa0i\" tokenTTL: \"0s\" #是kubeadm帮我们apiserver生成对外服务的证书用的。因为外部访问apiserver是通过负载均衡实现的，所以作为服务端提供的证书中应该写的hosts是负载均衡的地址。 apiServerCertSANs: #允许访问apiserver的地址 - 192.168.30.189 #apiserver的负载均衡ip，或者是slb的ip - k8s-master1 - k8s-slave1 - k8s-slave2 - 192.168.31.48 - 192.168.31.137 - 192.168.31.226 apiServerExtraArgs: apiserver-count: \"3\" endpoint-reconciler-type: lease EOF ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:2:4","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"calico安装 安装calico kubectl apply -f https://docs.projectcalico.org/v3.15/manifests/calico.yaml https://docs.projectcalico.org/manifests/calico-etcd.yaml kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 查看节点启动详情，calico3个进程都启动完毕 ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:3:0","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["K8S"],"content":"添加node节点 在另一台节点执行下面命令，加入集群 kubeadm join 192.168.31.226:6443 --token v7b8zo.5hc1au8vl96xqyie --discovery-token-ca-cert-hash sha256:908597386cd0311f24e2d7c95e40559e3137523078e69e1262643c6161abc10a 出现如下提示表示加入成功 在master执行kubectl get nodes，看到node状态都是Ready表示搭建完成 ","date":"2020-08-20","objectID":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/:4:0","tags":["k8s"],"title":"kubeadm搭建k8s 1.18","uri":"/kubeadm%E6%90%AD%E5%BB%BAk8s-1-18/"},{"categories":["Python"],"content":"本文主要对当前热门剧《隐秘的角落》进行豆瓣短评以及评价进行可视化分析，数据抓取主要为python编写的爬虫。 本文主要分为2个部分，分别对爬虫和可视化部分进行详解，本文脚本基于python 3.8.0版本进行编写 第三方依赖包 说明 pyecharts 百度开源可视化工具，用于生成图表和词云 jieba 国内比较好用的分词工具 requests python常用HTTP 库 lxml pyhon常用XML和HTML解析库 ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:0:0","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"数据抓取 ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:1:0","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"日志打印 为了方便调试和问题定位，单独引入了logging模块进行日志打印，为方便复用，日志部分单独写到了一个文件中，创建log.py文件 import logging import logging.handlers class Logger(object): level_relations = { 'debug':logging.DEBUG, 'info':logging.INFO, 'warning':logging.WARNING, 'error':logging.ERROR, 'crit':logging.CRITICAL }#日志级别关系映射 def __init__(self,filename,level='info',when='H',backCount=3,fmt='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'): #日志格式 #---------------------------- # %(asctime)s 年-月-日 时-分-秒，毫秒 # %(filename)s 文件名，不含目录 # %(pathname)s 目录名，完整路径 # %(funcName)s 函数名 # %(levelname)s 级别名 # %(lineno)d 行号 # %(module)s 模块名 # %(message)s 日志信息 # %(name)s 日志模块名 # %(process)d 进程id # %(processName)s 进程名 # %(thread)d 线程id # %(threadName)s 线程名 self.logger = logging.getLogger(filename) format_str = logging.Formatter(fmt)#设置日志格式 self.logger.setLevel(self.level_relations.get(level))#设置日志级别 sh = logging.StreamHandler()#往屏幕上输出 sh.setFormatter(format_str) #设置屏幕上显示的格式 th = logging.handlers.TimedRotatingFileHandler(filename=filename,when=when,backupCount=backCount,encoding='utf-8')#往文件里写入#指定间隔时间自动生成文件的处理器 #实例化TimedRotatingFileHandler #interval是时间间隔，backupCount是备份文件的个数，如果超过这个个数，就会自动删除，when是间隔的时间单位，单位有以下几种： # S 秒 # M 分 # H 小时、 # D 天、 # W 每星期（interval==0时代表星期一） # midnight 每天凌晨 th.setFormatter(format_str)#设置文件里写入的格式 self.logger.addHandler(sh) #把对象加到logger里 self.logger.addHandler(th) ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:1:1","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"爬虫 豆瓣对未登陆账号的请求有些限制，短评只能看到前面200条，所以采取了登陆账号的方式来请求更多的数据。豆瓣返回的数据为xml，因此采用了xpath的方式来获取短评数据，然后将数据存入本地文本中 class sprider(): def __init__(self): # 设置请求头 self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko'} self.short_comments_file = sys.path[0]+\"\\\\Short_comments_file.txt\" self.score_file = sys.path[0]+\"\\\\score_file.txt\" def login(self,session): login_url = 'https://accounts.douban.com/j/mobile/login/basic' ## 账号和密码需要修改为正确的 postData = {'ck': '', 'name': 'yourname', 'password': 'passwd', 'remember': 'false', 'ticket': ''} # 从网上看到，需要先get请求一次才能成功，不然能登录能200，但是不能进行后续的get请求抛403，具体原因不详 a = session.get(login_url,headers=self.headers) b = session.post(login_url, data=postData, headers=self.headers) if b.status_code == 200: logger.logger.info(\"登录成功\") else: # 登陆失败打印返回码和失败详情 logger.logger.error(\"登录失败:\"+str(b.status_code)+str(b.text)) sys.exit(48) def get_request(self,url,session): # get请求获取短评详情 request = session.get(url,headers=self.headers) if request.status_code == 200: # 防止乱码，将编码格式设置为utf-8 request.encode = 'utf-8' return request.text else: logger.logger.error('current status_code:'+str(request.status_code)+str(request.text)) sys.exit(44) def xpath_analysis(self, text, xpath): html = etree.HTML(text) # 使用xpath进行短评解析 result = html.xpath(xpath) return result def to_file(self, filename, data): # 采用追加的方式将内容写入文件 with open(filename, 'a', encoding='utf-8') as f: # xpath获取的data为列表，所以用遍历的方式写入 logger.logger.info(data) for i in data: f.write(i+'\\n') def init_file(self, filename): # 初始化文件，直接清空 with open(filename, 'w') as f: f.write('') ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:1:2","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"可视化 可视化部分主要是对前一步抓取到的数据进行清洗和分析，分词库用的是jieba。图表生成使用的是百度开源可视化工具pyecharts，本文只用了柱状图和词云，更多高级用法见pyecharts官方文档 class visualization(): def __init__(self): # 初始化文件路径 self.short_comments_file = sys.path[0]+\"\\\\Short_comments_file.txt\" self.score_file = sys.path[0]+\"\\\\score_file.txt\" self.path = sys.path[0] def analysis_score(self): evaluate = {} with open(self.score_file, 'r', encoding='utf-8') as f: for i in f.readlines(): i = re.sub(r\"[A-Za-z0-9\\-\\:]\", \"\", i) i = i.strip() # 去掉每行的换行符 # 如果这个没出现过，就初始化为1 if i not in evaluate: evaluate[i] = 1 logger.logger.debug(evaluate) else: # 如果已经出现过了，就在自加1 evaluate[i] += 1 logger.logger.debug(evaluate) logger.logger.info(evaluate) bar = Bar() eva = [] count = [] for k, v in evaluate.items(): if k != '': eva.append(k) count.append(v) bar.add_xaxis(eva) # 柱状图x轴 logger.logger.info('xaxis'+str(eva)) bar.add_yaxis(\"评价\", count) # 柱状图y轴 logger.logger.info('yaxis'+str(count)) bar.set_global_opts(title_opts=opts.TitleOpts(title=\"隐秘的角落 豆瓣评分\")) # 生成可视化图表 bar.render(self.path+\"\\\\score.html\") def analysis_short_comment(self): cut_words = \"\" for line in open(self.short_comments_file, 'r', encoding='utf-8'): line.strip('\\n') # 正则去掉标点等无效的字符，对数据进行清洗 line = re.sub(r\"[A-Za-z0-9\\：\\·\\—\\，\\。\\“ \\”\\....]\", \"\", line) # cut_all=False为精确模式，cut_all=True为全词模式 seg_list = jieba.cut(line, cut_all=False) cut_words += (\" \".join(seg_list)) all_words = cut_words.split() c = Counter() for x in all_words: if len(x) \u003e 1 and x != '\\r\\n': c[x] += 1 words = c.most_common(500) # 输出词频最高的前500词 logger.logger.debug(words) wordcloud = WordCloud() wordcloud.add(\"\", words, word_size_range=[5, 100], shape='circle') wordcloud.set_global_opts(title_opts=opts.TitleOpts(title=\"隐秘的角落 短评\")) wordcloud.render(self.path+\"\\\\short_comment.html\") ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:2:0","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"主函数 为了防止被豆瓣封IP，降低了采集频率采取了单线程和每次请求随机sleep 0.1s-4s的方式 def run_sprider(sprider): # 初始化文件 sprider.init_file(sprider.short_comments_file) sprider.init_file(sprider.score_file) # 遍历获取数据 s = requests.session() sprider.login(s) for page_start in range(0, 500, 20): # 范围从0-500，步长为20,页面上总评论数为500+ try: delay = round(random.uniform(0.1, 4), 1) logger.logger.info('i will sleep:'+str(delay)+'s') time.sleep(delay) URL = 'https://movie.douban.com/subject/33404425/comments?start={}\u0026limit=20\u0026sort=new_score\u0026status=P'.format( page_start) logger.logger.info('current_request_url:'+URL) x = sprider.get_request(URL,s) # 短评的xpath路径 xpath = '//*[@id=\"comments\"]/div[*]/div[2]/p/span/text()' short_comment = sprider.xpath_analysis(x, xpath) sprider.to_file(sprider.short_comments_file, short_comment) # 评分的xpath路径 xpath2 = '//*[@id=\"comments\"]/div[*]/div[2]/h3/span[2]/span[2]/@title' score = sprider.xpath_analysis(x, xpath2) sprider.to_file(sprider.score_file, score) except Exception as e: logger.logger.error(e) logger.logger.info('current_page:'+page_start) sys.exit(146) if __name__ == \"__main__\": # 数据抓取 sprider = sprider() run_sprider(sprider) # 数据分析和图标生成 c = visualization() c.analysis_score() c.analysis_short_comment() ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:3:0","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Python"],"content":"完整源码 log.py sprider.py ","date":"2020-07-05","objectID":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/:4:0","tags":["可视化","爬虫"],"title":"python分析豆瓣影评","uri":"/python%E5%88%86%E6%9E%90%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/"},{"categories":["Redis"],"content":"Redis3.0上加入了cluster模式，实现的redis的分布式存储，本文主要介绍Redis集群的相关搭建。 ","date":"2020-06-18","objectID":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:0:0","tags":["Redis","搭建文档"],"title":"Redis集群搭建","uri":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"集群特点 redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。 Redis-Cluster采用无中心结构,它的特点如下： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。 工作方式： 在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是cluster，可以理解为是一个集群管理的插件。当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点A1都宕机了，那么该集群就无法再提供服务了。 ","date":"2020-06-18","objectID":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:1:0","tags":["Redis","搭建文档"],"title":"Redis集群搭建","uri":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"集群规划 搭建redis最少需要6个节点。因虚拟机是2台，所以通过配置不同端口的方式，在第一台机器上启动3个节点，第二台上启动3个节点 IP 端口 192.168.31.226 6379 192.168.31.226 6380 192.168.31.226 6381 192.168.31.137 6379 192.168.31.137 6380 192.168.31.137 6381 ","date":"2020-06-18","objectID":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:0","tags":["Redis","搭建文档"],"title":"Redis集群搭建","uri":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"集群配置 本文默认认为已经会安装redis，搭建部分不再赘述。需要可参考之前文章Redis主从哨兵搭建安装部分 创建/usr/local/redis/conf_cluster目录，用于存放redis的配置文件。在conf_cluster目录下创建6379、6380、6381目录，修改好的配置文件redis.conf分别放到对应目录下 port 6379 # 端口6379 6380 6381 bind 192.168.31.226 # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes # redis后台运行 cluster-enabled yes # 开启集群 pidfile \"/usr/local/redis/conf_cluster/6379/redis.pid\" # pidfile文件对应6379 6380 6381 appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志 dir \"/usr/local/redis/conf_cluster\" # 设置redis数据写入目录 appendfilename appendonly.aof # aof日志文件名 logfile /tmp/redis-6379.log # redis日志路径对应6379 6380 6381 启动redis全部节点 /usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6379/redis.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6380/redis.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf_cluster/6381/redis.conf netstat -anp|grep tcp|grep -E '6379|6380|6381' #查看监听端口是否启动 ","date":"2020-06-18","objectID":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:3:0","tags":["Redis","搭建文档"],"title":"Redis集群搭建","uri":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"创建集群 用redis-cli创建整个redis集群(redis5以前的版本集群是依靠ruby脚本redis-trib.rb实现) /usr/local/redis/bin/redis-cli -a 1234 --cluster create --cluster-replicas 1 192.168.31.226:6379 192.168.31.226:6380 192.168.31.226:6381 192.168.31.137:6379 192.168.31.137:6380 192.168.31.137:6381 # -a 表示使用密码 --cluster-replicas 1表示为每个主节点创建一个副本 使用redlis-cli连接任意节点，cluster info 和 cluster nodes可查看当前集群状态，此时集群搭建成功 ","date":"2020-06-18","objectID":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:4:0","tags":["Redis","搭建文档"],"title":"Redis集群搭建","uri":"/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["Jenkins"],"content":"使用Jenkins进行持续集成，案例在虚拟机上进行搭建，本文主要介绍环境搭建以及配置部分。 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:0:0","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"准备工作 机器要求： 256 MB 内存，建议大于 512 MB 10 GB 的硬盘空间（用于 Jenkins 和 Docker 镜像） 需要安装以下软件 Java 8 ( JRE 或者 JDK 都可以) 首先从官网上下载Jenkins安装包，为方便使用这里我直接下载了war包进行安装 wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war 从国内源下载JDK 8,并且设置JAVA_HOME wget https://repo.huaweicloud.com/java/jdk/8u201-b09/jdk-8u201-linux-x64.tar.gz # 下载JDK安装包 echo 'export JAVA_HOME=/usr/java/jdk1.8.0_201 export PATH=$JAVA_HOME/bin:$PATH' \u003e\u003e/etc/profile # 配置java环境变量 source /etc/profile # 使配置的环境变量生效 java -version查看java版本,如果出现下图结果，表示JDK安装成功 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:1:0","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"使用Jenkins 执行java -jar jenkins.war --httpPort=8080启动jenkins，打开浏览器进入链接http://localhost:8080,访问jenkis ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:0","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"解锁Jenkins 第一次访问新的Jenkins实例时，系统会要求使用自动生成的密码对其进行解锁。 从Jenkins控制台日志输出中，复制自动生成的字母数字密码（在两组星号之间） ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:1","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"插件初始化 在解锁Jenkins页面上，将此密码粘贴到管理员密码字段中，然后单击继续,此时会进入插件安装界面,根据需要进行选择，这里我直接选择了自定义安装，暂时不安装任何插件，因为没有更改源，安装很慢 安装建议的插件 - 安装推荐的一组插件，这些插件基于最常见的用例. 选择要安装的插件 - 选择安装的插件集。当你第一次访问插件选择页面时，默认选择建议的插件。 插件下载完成后即可进入主页 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:2","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"主从搭建 先点击Manage Jenkins，然后点击Manage Nodes and Clouds 点击新建节点 填写slave相关信息,点击保存 在Configure Global Security中启用代理Java Web Start Agent Protocol/4 (TLS 加密)配置 然后根据提示，将agent.jar拷贝到slave节点目录，然后根据提示的命令运行slave 此时可以看到slave已经上线 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:3","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"用户权限配置 在一个部门内部，可能存在多个运维人员，而这些运维人员往往负责不同的项目，但是有可能他们用的又是同一个 Jenkins 的不同用户。那么我们就希望实现一个需求，能够不同的用户登录 Jenkins 以后看到不同的项目。由于jenkins默认的权限控制太过简陋，因此我们引入新的插件Role-based Authorization Strategy 插件安装 首先因国内直连jenkens的插件中心速度很慢，因此先修改源站为清华大学地址https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 然后搜索插件名进行安装 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:4","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"权限配置 重启Jenkins以后，再度打开Configure Global Security会发现多了我们刚刚插件的选项，选择Role-Based Strategy，点击保存 这时在Manage Jenkins就会多出一个选项，Manage and Assign Roles 在Manage Roles进行角色以及相关权限添加，有Global roles、Item roles以及Node roles可以分别进行配置 然后点击Assign Roles，进行相关用户的角色授予 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:5","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"创建JOB 点击新建job，创建一个Freestyle project的JOB 点击新创建的JOB进行配置，源码管理选择GIT(需要提前安装git插件才会出来这个选项，填入git库地址，以及相关账号及密码) 定时触发器可根据实际情况进行选择，然后配置构建步骤点击保存即可，我本地是部署nginx静态页面，因此直接执行shell命令进行部署 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:6","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["Jenkins"],"content":"触发构建 点击立即构建，即可触发构建任务，本次部署为和jenkins同一台机器上的nginx静态页面 点击控制台输出，可以查看构建过程中的具日志 ","date":"2020-06-10","objectID":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/:2:7","tags":["jenkins","持续集成","搭建文档"],"title":"使用Jenkins进行持续集成","uri":"/%E4%BD%BF%E7%94%A8jenkins%E8%BF%9B%E8%A1%8C%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"categories":["k8s"],"content":"本文介绍了k8s常见2中方案的差异，以及各自的优势 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:0:0","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"Calico ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:1:0","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"什么是Calico Calico 是一个三层的数据中心网络方案,而且方便集成 OpenStack 这种 IaaS 云架构,能够提供高效可控的 VM、容器、裸机之间的通信。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:1:1","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"Calico网络基本架构 Calico BGP模式在小规模集群中可以直接互联,在大规模集群中可以通过额外的BGP route reflector来完成。 Calico是一个基于BGP的纯三层的网络方案,与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。Calico在每个计算节点都利用Linux Kernel实现了一个高效的vRouter来负责数据转发。每个vRouter都通过BGP1协议把在本节点上运行的容器的路由信息向整个Calico网络广播,并自动设置到达其他节点的路由转发规则。Calico利用了Linux内核原生的路由和iptables防火墙功能。 进出各个容器、虚拟机和物理主机的所有流量都会在路由到目标之前遍历这些内核规则。 主要组件 Felix：Calico agent,跑在每台需要运行workload的节点上,主要负责配置路由及ACLs等信息来确保endpoint的连通状态； etcd：分布式键值存储,主要负责网络元数据一致性,确保Calico网络状态的准确性； BGPClient(BIRD)：主要负责把Felix写入kernel的路由信息分发到当前Calico网络,确保workload间的通信的有效性； BGP Route Reflector(BIRD)：大规模部署时使用,摒弃所有节点互联的mesh模式,通过一个或者多个BGPRoute Reflector来完成集中式的路由分发； CalicoCtl：允许从命令行界面配置实现高级策略和网络。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:1:2","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"Flannel ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:2:0","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"什么是Flannel Flannel是由CoreOS开发的项目,可能是最直接和最受欢迎的CNI插件。它是容器编排系统中最成熟的网络结构示例之一,旨在实现更好的容器间和主机间网络。随着CNI概念的兴起,Flannel CNI插件算是早期的入门。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:2:1","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"Flannel网络基本架构 Flannel首先创建了一个名为Flannel0的网桥,而且这个网桥的一端连接docker0网桥,另一端连接一个叫作Flanneld的服务进程。Flanneld进程上连etcd,利用etcd来管理可分配的IP地址段资源,同时监控etcd中每个Pod的实际地址,并在内存中建立了一个Pod节点路由表；Flanneld进程下连docker0和物理网络,使用内存中的Pod节点路由表,将docker0发给它的数据包包装起来,利用物理网络的连接将数据包投递到目标Flanneld上,从而完成Pod到Pod之间的直接地址通信。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:2:2","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"对比 Calico整个过程中始终都是根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和Flannel比起来效率就会快多了。 由于Flannel几乎是最早的跨网络通信解决方案，其他的方案都可以被看做是Fannel的某种改进版。 Calico的设计比较新颖，Flannel的Host-Gateway模式之所以不能跨二层网络，是因为它只能修改主机的路由，Calico把改路由表的做法换成了标准的BGP路由协议。 相当于在每个节点上模拟出一个额外的路由器，由于采用的是标准协议，Calico模拟路由器的路由表信息就可以被传播到网络的其他路由设备中，这样就实现了在三层网络上的高速跨节点网络。 不过在现实中的网络并不总是支持BGP路由的，因此Calico也设计了一种IPIP模式，使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中的，因此它的速度理论上比VxLAN快一点点，但安全性更差。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:3:0","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["k8s"],"content":"K8S为什么会出现各种网络方案 由于Docker等容器工具只是利用内核的网络Namespace实现了网络隔离，各个节点上的容器是在所属节点上自动分配IP地址的，从全局来看，这种局部地址就像是不同小区里的门牌号，一旦拿到一个更大的范围上看，就可能是会重复的。 为了解决这个问题，Flannel设计了一种全局的网络地址分配机制，即使用Etcd来存储网段和节点之间的关系，然后Flannel配置各个节点上的Docker（或其他容器工具），只在分配到当前节点的网段里选择容器IP地址。 ","date":"2020-06-10","objectID":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/:4:0","tags":["k8s"],"title":"calico网络和flannel对比","uri":"/calico%E7%BD%91%E7%BB%9C%E5%92%8Cflannel%E5%AF%B9%E6%AF%94/"},{"categories":["Redis"],"content":"本文章基于最新版本Redis5.0.5版本进行redis主从哨兵模式搭建 ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:0:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"Redis 简介 Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件. 它支持多种类型的数据结构，如字符串（strings），散列（hashes），列表（lists），集合（sets），有序集合（sorted sets）与范围查询，bitmaps，hyperloglogs 和 地理空间（geospatial）索引半径查询. Redis 内置了复制（replication），LUA脚本（Lua scripting），LRU驱动事件（LRU eviction），事务（transactions）和不同级别的磁盘持久化（persistence），并通过Redis哨兵（Sentinel）和自动分区（Cluster）提供高可用性（high availability）.为了实现其卓越的性能，Redis采用运行在内存中的数据集工作方式.根据您的使用情况，您可以每隔一定时间将数据集导出到磁盘，或者追加到命令日志中. 您也可以关闭持久化功能，将Redis作为一个高效的网络的缓存数据功能使用. ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:1:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"集群规划 搭建redis需要1个主节点，且还有2个从节点，所以至少需要配置3个节点。因虚拟机是2台，所以通过配置不同端口的方式，在第一台机器上启动1个主节点，第二台上启动2个从节点，3个节点上分别启动sentinel ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:2:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"源码编译安装 首先进入redis官网下载最新版本的redis，源码编译安装,源码安装需要依赖gcc、gcc-c++，如果出现gcc：命令未找到等错误，请先安装对应的依赖 wget http://download.redis.io/releases/redis-5.0.5.tar.gz tar -zvxf redis-5.0.5.tar.gz make MALLOC=libc #虚拟机只分配了一核所以未使用-j参数 mkdir -p /usr/local/redis #创建redis安装目录 make PREFIX=/usr/local/redis install redis启动的时候会占用一个终端，这是因为没有指定redis.conf文件，启动的时候是按默认进行的。所以如果不想使其占用，我们可以修改 redis.conf 配置文件，修改daemonize no 为 daemonize yes，然后再指定配置文件启动redis服务 然后启动redis进程 /usr/local/redis/bin/redis-server /usr/local/redis/conf/redis.conf 查看端口和使用redis-cli连接redis，启动正常 ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:3:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"集群配置 ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:4:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"master 创建/usr/local/redis/conf目录，用于存放redis的配置文件。在conf目录下创建6379目录，修改好的配置文件redis.conf放到目录下 port 6379 # 端口6379 bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes # redis后台运行 pidfile /var/run/redis_6379.pid # pidfile文件对应6379 appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志 appendfilename appendonly.aof # aof日志文件名 logfile /tmp/redis-6379.log # redis日志路径 先kill掉原来验证的单节点进程，然后启动master /usr/local/redis/bin/redis-server /usr/local/redis/conf/6379/redis.conf netstat -anp|grep tcp|grep -E '6379' #查看监听端口是否启动 ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:4:1","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"slave 创建/usr/local/redis/conf目录，用于存放redis的配置文件。在conf目录下创建6379、6380目录，修改好的配置文件redis.conf放到目录下 port 6379 # 端口6379、6380 bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes # redis后台运行 pidfile /var/run/redis_6379.pid # pidfile文件对应6379、6380 appendonly yes # aof日志开启 有需要就开启，它会每次写操作都记录一条日志 appendfilename appendonly.aof # aof日志文件名 logfile /tmp/redis-6379.log # redis日志路径 replicaof 192.168.31.137 6379 #master节点的ip和端口 然后启动slave /usr/local/redis/bin/redis-server /usr/local/redis/conf/6379/redis.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6380/redis.conf ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:4:2","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"sentinel 复制安装包中的sentinel.conf文件到redis的配置文件目录下，修改配置文件,将配置文件分别复制到对应的目录 port 26379 # 端口26379、26380 bind 本机ip # 默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes # redis后台运行 pidfile /var/run/sentinel_26379.pid # pidfile文件对应6379、6380 logfile /tmp/redis-26379.log # redis日志路径 sentinel down-after-milliseconds mymaster 30000 #master或slave多长时间（默认30秒）不能使用后标记为s_down状态 sentinel monitor mymaster 192.168.31.137 6379 2 #监听的master的集群名和节点，这个后面的数字2,是指当有两个及以上的sentinel服务检测到master宕机，才会去执行主从切换的功能 sentinel parallel-syncs mymaster 1 #指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为replication而不可用 启动sentinel /usr/local/redis/bin/redis-sentinel /usr/local/redis/conf/6379/sentinel.conf /usr/local/redis/bin/redis-sentinel /usr/local/redis/conf/6380/sentinel.conf 查看sentinel启动状态 ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:4:3","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Redis"],"content":"故障迁移测试 连接redis集群/usr/local/redis/bin/redis-cli -c -h 192.168.31.226 -p 6379,查询当前master信息，当前集群有2个slave 模拟当前master故障，杀掉master进程 再次连接redis集群发现master已经进行了重新选举，选出了新的master ","date":"2020-06-06","objectID":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/:5:0","tags":["Redis","搭建文档"],"title":"Redis主从哨兵搭建","uri":"/redis%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"本文主要介绍prometheus+grafana+consul方案监控系统的搭建，本次搭建采用虚拟机方式进行搭建，采取consul进行服务注册 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:0:0","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"prometheus安装与配置 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:1:0","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"介绍与基本架构 prometheus是由谷歌研发的一款开源的监控软件，目前已经被云计算本地基金会托管，是继k8s托管的第二个项目。 prometheus根据配置定时去拉取各个节点的数据，默认使用的拉取方式是pull，也可以使用pushgateway提供的push方式获取各个监控节点的数据。将获取到的数据存入TSDB，一款时序型数据库。此时prometheus已经获取到了监控数据，可以使用内置的PromQL进行查询。它的报警功能使用Alertmanager提供，Alertmanager是prometheus的告警管理和发送报警的一个组件。prometheus原生的图标功能过于简单，可将prometheus数据接入grafana，由grafana进行统一管理。 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:1:1","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"安装Prometheus Server 首先从官网下载Prometheus安装程序 wget https://github.com/prometheus/prometheus/releases/download/v2.18.1/prometheus-2.18.1.linux-amd64.tar.gz 执行执行解压命令 tar -vxf prometheus-2.18.1.linux-amd64.tar.gz 编辑解压目录下的prometheus.yml，执行命令：vi prometheus.yml进行基本配置 [root@guohailan1 prometheus-2.18.1.linux-amd64]# cat prometheus.yml # my global config global: scrape_interval: 10s #每10s采集一次数据 evaluation_interval: 10s #每10s做一次告警检测 scrape_timeout: 5s #拉取一个 target 的超时时间 alerting: #Alertmanager 相关配置暂时未配置 alertmanagers: - static_configs: - targets: rule_files: scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'linux-exporter' metrics_path: /metrics static_configs: - targets: ['192.168.31.48:9100'] 配置开机启动，在CentOS8下官方推荐使用systemctl进行开机自启管理 cat \u003c\u003c EOF \u003e /usr/lib/systemd/system/prometheus.service [Unit] Description=Prometheus After=network.target [Service] Type=simple ExecStart=/bin/bash -c \"/prometheus/prometheus-2.18.1.linux-amd64/prometheus --web.enable-lifecycle --storage.local.retention 24h0m0s --config.file=/prometheus/prometheus-2.18.1.linux-amd64/prometheus.yml\" [Install] WantedBy=multi-user.target EOF 设置自启动和启动prometheus systemctl enable prometheus systemctl start prometheus 浏览器端访问http://192.168.31.48:9090/graph，如果出现界面表示搭建成功,如果服务器上有防火墙，可能需要先关闭防火墙或者配置规则 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:1:2","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"node_exporter安装 node-exporter用于采集服务器层面的运行指标，包括机器的loadavg、filesystem、meminfo等基础监控，类似于传统主机监控维度的zabbix-agent。node-export由prometheus官方提供、维护，不会捆绑安装，但基本上是必备的exporter。 从prometheus官网下载相应版本的node_exporter wget https://github.com/prometheus/consul_exporter/releases/download/v0.6.0/consul_exporter-0.6.0.linux-amd64.tar.gz 配置开机启动 cat \u003c\u003c EOF \u003e /usr/lib/systemd/system/node_exporter.service [Unit] Description=node_exporter After=network.target [Service] Type=simple ExecStart=/bin/bash -c \"/root/exporter/node_exporter-1.0.0-rc.1.linux-amd64/node_exporter\" [Install] WantedBy=multi-user.target EOF ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:2:0","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"consul ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:3:0","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"安装consul Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.命令行超级好用的虚拟机管理软件 vgrant 也是 HashiCorp 公司开发的产品.一致性协议采用 Raft 算法,用来保证服务的高可用. 使用 GOSSIP 协议管理成员和广播消息, 并且支持 ACL 访问控制. 首先从官网上下载consul最新版本并且解压 wget https://releases.hashicorp.com/consul/1.7.3/consul_1.7.3_linux_amd64.zip unzip consul_1.7.3_linux_amd64.zip 本案例搭建为单机版本,将下列命令写入start.sh consul agent -data-dir /prometheus/consul/data -bind=172.0.0.1 -datacenter=dc1 -ui -client=0.0.0.0 -server -http-port=8500 -bootstrap-expect=1 配置开机启动 cat \u003c\u003c EOF \u003e /usr/lib/systemd/system/consul.service [Unit] Description=consul After=network.target [Service] Type=simple ExecStart=/bin/bash -c \"/prometheus/consul/start.sh\" [Install] WantedBy=multi-user.target EOF 访问http://localhost:8500 ,能出现界面表示搭建成功 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:3:1","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"consul注册与删除 在服务器上使用如下命令将node_exporter,注册成功后可以看到consul界面上出现了注册的信息，Node Checks和Service Checks绿色表示node_exporter状态正常，点击可查看详情 curl -X PUT -d '{\"id\": \"192.168.31.48\",\"name\": \"node-exporter\",\"address\": \"192.168.31.48\",\"port\": 9100,\"tags\": [\"guohailan1\"],\"checks\": [{\"http\": \"http://192.168.31.48:9100/metrics\", \"interval\": \"5s\"}]}' http://192.168.31.48:8500/v1/agent/service/register 如果注册错误或者不使用了可用如下命令删除注册信息 curl -X PUT http://192.168.31.48:8500/v1/agent/service/deregister/node-exporter ###prometheus对接consul 修改prometheus配置文件vi /prometheus/prometheus-2.18.1.linux-amd64/prometheus.yml # my global config global: scrape_interval: 10s evaluation_interval: 10s scrape_timeout: 5s alerting: alertmanagers: - static_configs: - targets: rule_files: scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'consul-prometheus' consul_sd_configs: - server: '192.168.31.48:8500' #consul地址 services: [] relabel_configs: - source_labels: [__meta_consul_service_port] #prometheus将对匹配上的lables进行操作 regex: 9100 action: keep 详细 relabel_configs 配置及说明可以参考 relabel_config 官网说明，这里我简单列举一下里面每个 relabel_action 的作用，方便下边演示。 replace: 根据 regex 的配置匹配 source_labels 标签的值（注意：多个 source_label 的值会按照 separator 进行拼接），并且将匹配到的值写入到 target_label 当中，如果有多个匹配组，则可以使用 ${1}, ${2} 确定写入的内容。如果没匹配到任何内容则不对 target_label 进行重新， 默认为 replace。 keep: 丢弃 source_labels 的值中没有匹配到 regex 正则表达式内容的 Target 实例 drop: 丢弃 source_labels 的值中匹配到 regex 正则表达式内容的 Target 实例 hashmod: 将 target_label 设置为关联的 source_label 的哈希模块 labelmap: 根据 regex 去匹配 Target 实例所有标签的名称（注意是名称），并且将捕获到的内容作为为新的标签名称，regex 匹配到标签的的值作为新标签的值 labeldrop: 对 Target 标签进行过滤，会移除匹配过滤条件的所有标签 labelkeep: 对 Target 标签进行过滤，会移除不匹配过滤条件的所有标签 配置完成重启promethus即可看到targets上有新的node_exporter信息，其中Labels中的指标即上一步配置中的 “source_labels” ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:3:2","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"grafana grafana是一个非常酷的数据可视化平台，常常应用于显示监控数据，底层数据源可以支持influxDb、graphite、elasticSeach。 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:4:0","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"安装grafana 首先还是从官网下载安装包,进行安装 wget https://dl.grafana.com/oss/release/grafana-7.0.1-1.x86_64.rpm sudo yum install grafana-7.0.1-1.x86_64.rpm systemctl enable grafana-server.service #允许开机启动 systemctl start grafana-server.service #启动grafana 启动服务之后：http://localhost:3000 。用户名和密码在初始化都是admin和admin ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:4:1","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"},{"categories":["Prometheus"],"content":"配置grafana 数据源 首先配置数据源,选择之前搭建的prometheus的连接串，点击保存 dashboard grafana官网有很多大神的作品，可以直接使用 grafana支持很多中导入方式，上传json文件或者直接贴json配置，这里因可以使用外网，直接复制模板ID，导入到grafana中 导入完成后，即可看到报表信息 ","date":"2020-06-02","objectID":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/:4:2","tags":["prometheus","grafana","监控","搭建文档"],"title":"prometheus+grafana监控搭建","uri":"/prometheus-grafana%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/"}]